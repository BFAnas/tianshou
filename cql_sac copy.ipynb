{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/glfw/__init__.py:916: GLFWError: (65544) b'X11: The DISPLAY environment variable is missing'\n",
      "  warnings.warn(message, GLFWError)\n",
      "/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/gymnasium/envs/registration.py:596: UserWarning: \u001b[33mWARN: plugin: shimmy.registration:register_gymnasium_envs raised Traceback (most recent call last):\n",
      "  File \"/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/gymnasium/envs/registration.py\", line 594, in load_plugin_envs\n",
      "    fn()\n",
      "  File \"/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/shimmy/registration.py\", line 262, in register_gymnasium_envs\n",
      "    _register_dm_control_envs()\n",
      "  File \"/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/shimmy/registration.py\", line 26, in _register_dm_control_envs\n",
      "    from shimmy.dm_control_compatibility import DmControlCompatibilityV0\n",
      "  File \"/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/shimmy/dm_control_compatibility.py\", line 12, in <module>\n",
      "    import dm_env\n",
      "ModuleNotFoundError: No module named 'dm_env'\n",
      "\u001b[0m\n",
      "  logger.warn(f\"plugin: {plugin.value} raised {traceback.format_exc()}\")\n",
      "<frozen importlib._bootstrap>:283: DeprecationWarning: the load_module() method is deprecated and slated for removal in Python 3.12; use exec_module() instead\n",
      "Warning: Mujoco-based envs failed to import. Set the environment variable D4RL_SUPPRESS_IMPORT_ERROR=1 to suppress this message.\n",
      "No module named 'mjrl'\n",
      "Warning: Flow failed to import. Set the environment variable D4RL_SUPPRESS_IMPORT_ERROR=1 to suppress this message.\n",
      "No module named 'flow'\n",
      "Warning: FrankaKitchen failed to import. Set the environment variable D4RL_SUPPRESS_IMPORT_ERROR=1 to suppress this message.\n",
      "No module named 'dm_env'\n",
      "Warning: CARLA failed to import. Set the environment variable D4RL_SUPPRESS_IMPORT_ERROR=1 to suppress this message.\n",
      "No module named 'carla'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "from tianshou.data import Collector, Batch, to_torch\n",
    "from tianshou.data.types import RolloutBatchProtocol\n",
    "from tianshou.data.buffer.vecbuf import VectorReplayBuffer, ReplayBuffer\n",
    "from tianshou.env import SubprocVectorEnv\n",
    "from tianshou.policy import SACPolicy, BasePolicy\n",
    "from tianshou.utils.net.common import Net\n",
    "from tianshou.utils.net.continuous import ActorProb, Critic\n",
    "from examples.offline.utils import load_buffer_d4rl\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tianshou.utils import TensorboardLogger\n",
    "from tianshou.trainer import OffpolicyTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sac_args():\n",
    "    args = argparse.Namespace(\n",
    "        task=\"Hopper-v2\",\n",
    "        buffer_size=1000000,\n",
    "        hidden_sizes=[256, 256, 256],\n",
    "        actor_lr=1e-4,\n",
    "        critic_lr=3e-4,\n",
    "        gamma=0.99,\n",
    "        tau=0.005,\n",
    "        alpha_lr=1e-4,\n",
    "        alpha=0.36,\n",
    "        start_timesteps=1,\n",
    "        epoch=200,\n",
    "        step_per_epoch=5000,\n",
    "        step_per_collect=1,\n",
    "        update_per_step=1,\n",
    "        batch_size=256,\n",
    "        training_num=1,\n",
    "        test_num=10,\n",
    "        device=device,\n",
    "        norm_layer=True,\n",
    "    )\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/gymnasium/envs/registration.py:513: DeprecationWarning: \u001b[33mWARN: The environment Hopper-v2 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
      "  logger.deprecation(\n",
      "/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/gymnasium/envs/mujoco/mujoco_env.py:211: DeprecationWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n",
      "  logger.deprecation(\n"
     ]
    }
   ],
   "source": [
    "args = get_sac_args()\n",
    "env = gym.make(args.task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_policy(path):\n",
    "    args = get_sac_args()\n",
    "    env = gym.make(args.task)\n",
    "    args.state_shape = env.observation_space.shape or env.observation_space.n\n",
    "    args.action_shape = env.action_space.shape or env.action_space.n\n",
    "    args.max_action = env.action_space.high[0]\n",
    "    # model\n",
    "    net_a = Net(args.state_shape, hidden_sizes=args.hidden_sizes, device=args.device)\n",
    "    actor = ActorProb(\n",
    "        net_a,\n",
    "        args.action_shape,\n",
    "        device=args.device,\n",
    "        unbounded=True,\n",
    "        conditioned_sigma=True,\n",
    "    ).to(args.device)\n",
    "    actor_optim = torch.optim.Adam(actor.parameters(), lr=args.actor_lr)\n",
    "    net_c1 = Net(\n",
    "        args.state_shape,\n",
    "        args.action_shape,\n",
    "        hidden_sizes=args.hidden_sizes,\n",
    "        concat=True,\n",
    "        device=args.device,\n",
    "        norm_layer=nn.LayerNorm if args.norm_layer else None\n",
    "    )\n",
    "    net_c2 = Net(\n",
    "        args.state_shape,\n",
    "        args.action_shape,\n",
    "        hidden_sizes=args.hidden_sizes,\n",
    "        concat=True,\n",
    "        device=args.device,\n",
    "        norm_layer=nn.LayerNorm if args.norm_layer else None\n",
    "    )\n",
    "    critic1 = Critic(net_c1, device=args.device).to(args.device)\n",
    "    critic1_optim = torch.optim.Adam(critic1.parameters(), lr=args.critic_lr)\n",
    "    critic2 = Critic(net_c2, device=args.device).to(args.device)\n",
    "    critic2_optim = torch.optim.Adam(critic2.parameters(), lr=args.critic_lr)\n",
    "    target_entropy = -np.prod(env.action_space.shape)\n",
    "    log_alpha = torch.tensor([np.log(args.alpha)], requires_grad=True, device=device)\n",
    "    alpha_optim = torch.optim.Adam([log_alpha], lr=args.alpha_lr)\n",
    "    alpha = (target_entropy, log_alpha, alpha_optim)\n",
    "    policy = SACPolicy(\n",
    "        actor,\n",
    "        actor_optim,\n",
    "        critic1,\n",
    "        critic1_optim,\n",
    "        critic2,\n",
    "        critic2_optim,\n",
    "        tau=args.tau,\n",
    "        gamma=args.gamma,\n",
    "        alpha=alpha,\n",
    "        action_space=env.action_space,\n",
    "    )\n",
    "    policy.load_state_dict(torch.load(path, map_location=args.device))\n",
    "    print(\"Loaded agent from: \", path)\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded agent from:  /data/user/R901105/dev/log/Hopper-v2/cql/0/231220-111219/policy.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/gymnasium/envs/registration.py:513: DeprecationWarning: \u001b[33mWARN: The environment Hopper-v2 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
      "  logger.deprecation(\n",
      "/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/gymnasium/envs/mujoco/mujoco_env.py:211: DeprecationWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n",
      "  logger.deprecation(\n"
     ]
    }
   ],
   "source": [
    "cql_policy = load_policy(\"/data/user/R901105/dev/log/Hopper-v2/cql/0/231220-111219/policy.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded agent from:  /data/user/R901105/dev/log/Hopper-v2/sac/0/231219-163624/policy.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/gymnasium/envs/registration.py:513: DeprecationWarning: \u001b[33mWARN: The environment Hopper-v2 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
      "  logger.deprecation(\n",
      "/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/gymnasium/envs/mujoco/mujoco_env.py:211: DeprecationWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n",
      "  logger.deprecation(\n"
     ]
    }
   ],
   "source": [
    "sac_policy = load_policy(\"/data/user/R901105/dev/log/Hopper-v2/sac/0/231219-163624/policy.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = SubprocVectorEnv([lambda: gym.make(args.task) for _ in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n/ep': 40,\n",
       " 'n/st': 26003,\n",
       " 'rews': array([1486.25269722, 1823.26179735, 1815.41471127, 1823.64823606,\n",
       "        2087.71584701, 2396.16515738, 2461.0877074 , 2686.45698862,\n",
       "        2770.80036959, 3384.23060393, 1842.48922955, 1866.94245461,\n",
       "        1898.13672459, 1807.75406455, 1417.38894988, 1665.01841421,\n",
       "        2181.79117767, 1902.67316167, 3142.5463871 , 1484.83423753,\n",
       "        2180.77193707, 2174.34573781, 1804.64465422, 2182.29790974,\n",
       "        1899.60368569, 2745.06289326, 2078.45634837, 2320.67582828,\n",
       "        1856.80881647, 2773.04795628, 2183.64806295, 2268.10985287,\n",
       "        1907.08551018, 1816.56290292, 1843.81815116, 2998.18710922,\n",
       "        1994.10289773, 2978.34000772, 2359.09940191, 2161.83989175]),\n",
       " 'lens': array([455, 546, 551, 551, 633, 717, 745, 798, 829, 998, 551, 562, 575,\n",
       "        546, 436, 505, 655, 575, 938, 454, 657, 655, 548, 656, 575, 822,\n",
       "        619, 696, 556, 833, 659, 683, 574, 551, 557, 891, 600, 890, 711,\n",
       "        650]),\n",
       " 'idxs': array([1, 7, 4, 6, 2, 3, 0, 8, 5, 9, 1, 4, 6, 2, 0, 8, 3, 5, 7, 4, 9, 1,\n",
       "        0, 2, 8, 6, 3, 5, 4, 7, 1, 9, 8, 6, 3, 0, 5, 2, 4, 7]),\n",
       " 'rew': 2161.777961820364,\n",
       " 'len': 650.075,\n",
       " 'rew_std': 461.91812193631097,\n",
       " 'len_std': 134.4930086472899}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cql_policy.eval()\n",
    "cql_collector = Collector(cql_policy, envs)\n",
    "cql_result = cql_collector.collect(n_episode=40)\n",
    "cql_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n/ep': 40,\n",
       " 'n/st': 38521,\n",
       " 'rews': array([1650.7395058 , 2593.72564536, 3602.29768638, 3553.53303002,\n",
       "        3580.08063385, 3579.30580244, 3520.1980104 , 3553.8322047 ,\n",
       "        3550.06542435, 3569.61099467, 3607.30669895, 3578.27491201,\n",
       "        3528.71862059, 3586.13532283, 3559.06398147, 3580.10912595,\n",
       "        3571.73881482, 3570.89898834, 3521.56364915, 3546.21762465,\n",
       "        3533.51493836, 2312.71887543, 3524.73381178, 3234.52668798,\n",
       "        3129.11860956, 3535.58325009, 3591.21120569, 3570.74677282,\n",
       "        3568.84516274, 3556.83502317, 3581.03732314, 3564.99177602,\n",
       "        3575.07730797, 3533.45569568, 3550.81724913, 3535.46933535,\n",
       "        3562.03007878, 3568.32463834, 3497.97434734, 3642.65175359]),\n",
       " 'lens': array([ 478,  702, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000,\n",
       "        1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000,  641,\n",
       "         962,  861,  877, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000,\n",
       "        1000, 1000, 1000, 1000, 1000, 1000, 1000]),\n",
       " 'idxs': array([8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 0,\n",
       "        9, 2, 4, 1, 3, 5, 6, 7, 8, 0, 9, 2, 4, 1, 3, 5, 6, 7]),\n",
       " 'rew': 3439.3270129926577,\n",
       " 'len': 963.025,\n",
       " 'rew_std': 383.122834323531,\n",
       " 'len_std': 108.55608861321414}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sac_policy.eval()\n",
    "sac_collector = Collector(sac_policy, envs)\n",
    "sac_result = sac_collector.collect(n_episode=40)\n",
    "sac_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixedPolicy(BasePolicy):\n",
    "    def __init__(self, base_policy: BasePolicy, expert_policy: BasePolicy, action_space, buffer):\n",
    "        super().__init__(action_space=action_space, action_scaling=True)\n",
    "        self.base_policy = base_policy\n",
    "        self.expert_policy = expert_policy\n",
    "        self.device = device\n",
    "        self.buffer = buffer\n",
    "\n",
    "    def forward(self, batch: RolloutBatchProtocol, state=None, **kwargs):\n",
    "        bsz = len(batch.obs)\n",
    "        batch = to_torch(batch, dtype=torch.float32, device=self.device)\n",
    "        self.base_policy.eval()\n",
    "        self.expert_policy.eval()\n",
    "        with torch.no_grad():\n",
    "            expert_result = self.expert_policy(batch)\n",
    "            base_result = self.base_policy(batch)\n",
    "            expert_qvalues1 = self.expert_policy.critic1(batch.obs, expert_result.act)\n",
    "            expert_qvalues2 = self.expert_policy.critic2(batch.obs, expert_result.act)\n",
    "            expert_qvalues = torch.minimum(expert_qvalues1, expert_qvalues2)\n",
    "            base_qvalues1 = self.base_policy.critic1(batch.obs, base_result.act)\n",
    "            base_qvalues2 = self.base_policy.critic2(batch.obs, base_result.act)\n",
    "            base_qvalues = torch.minimum(base_qvalues1, base_qvalues2)\n",
    "        cede_ctrl = base_qvalues < expert_qvalues\n",
    "        actions = torch.where(cede_ctrl, expert_result.act, base_result.act)\n",
    "        return Batch(**{'act': actions, 'policy': Batch({'cede_ctrl': cede_ctrl})})\n",
    "\n",
    "    def train(self, mode: bool = True) -> \"MixedPolicy\":\n",
    "        self.base_policy.train(mode)\n",
    "        return self\n",
    "    \n",
    "    def process_fn(self, batch: RolloutBatchProtocol, buffer: ReplayBuffer, indices: np.ndarray) -> RolloutBatchProtocol:\n",
    "        return self.base_policy.process_fn(batch, buffer, indices)\n",
    "\n",
    "    def learn(self, batch, **kwargs):\n",
    "        # cede_ctrl = batch.policy.cede_ctrl.cpu().squeeze()\n",
    "        # train_batch = batch[~cede_ctrl]\n",
    "        # info = self.base_policy.learn(train_batch)\n",
    "        info = self.base_policy.learn(batch)\n",
    "        return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_buffer = VectorReplayBuffer(5000, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_policy = MixedPolicy(cql_policy, sac_policy, env.action_space, test_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n/ep': 40,\n",
       " 'n/st': 39442,\n",
       " 'rews': array([2877.85420345, 3579.38976417, 3566.2887685 , 3612.26257751,\n",
       "        3648.73333248, 3655.45149677, 3559.89113674, 3552.6419363 ,\n",
       "        3588.0665408 , 3578.10143437, 3576.98877615, 3561.46943533,\n",
       "        3558.70658515, 3598.58161738, 3558.16991895, 3525.51417879,\n",
       "        3579.98865515, 3558.74737203, 3518.58784384, 3641.33416581,\n",
       "        3563.76800552, 3231.92112228, 3558.71492095, 3535.73037386,\n",
       "        3558.96929922, 3583.61728139, 3556.20513785, 3542.30987626,\n",
       "        3584.5683532 , 3573.77252436, 2903.35971768, 3574.50541953,\n",
       "        3551.28549583, 3540.74278544, 3513.75433021, 3546.94601085,\n",
       "        3569.82593565, 3520.37927402, 3546.47417846, 3562.03266267]),\n",
       " 'lens': array([ 798, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000,\n",
       "        1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000,  869,\n",
       "        1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000,  775, 1000, 1000,\n",
       "        1000, 1000, 1000, 1000, 1000, 1000, 1000]),\n",
       " 'idxs': array([8, 0, 1, 2, 3, 4, 5, 6, 7, 9, 8, 0, 1, 2, 3, 4, 5, 6, 7, 9, 8, 0,\n",
       "        1, 2, 3, 4, 5, 6, 7, 9, 4, 8, 0, 1, 2, 3, 5, 6, 7, 9]),\n",
       " 'rew': 3525.391311123465,\n",
       " 'len': 986.05,\n",
       " 'rew_std': 157.82384420468222,\n",
       " 'len_std': 50.20107070571304}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixed_collector = Collector(mixed_policy, envs)\n",
    "mixed_result = mixed_collector.collect(n_episode=40)\n",
    "mixed_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeterministicPolicy(BasePolicy):\n",
    "    def __init__(self, policy, action_space):\n",
    "        super().__init__(action_space=action_space, action_scaling=True)\n",
    "        self.policy = policy\n",
    "\n",
    "    def train(self, mode: bool = True) -> \"DeterministicPolicy\":\n",
    "        self.policy.train(mode)\n",
    "        return self\n",
    "    \n",
    "    def forward(self, batch, state=None, **kwargs):\n",
    "        self.policy.eval()\n",
    "        return self.policy(batch)\n",
    "    \n",
    "    def learn(self, batch, **kwargs):\n",
    "        info = self.policy.learn(batch)\n",
    "        return info\n",
    "    \n",
    "    def process_fn(self, batch: RolloutBatchProtocol, buffer: ReplayBuffer, indices: np.ndarray) -> RolloutBatchProtocol:\n",
    "        return self.policy.process_fn(batch, buffer, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "det_cql = DeterministicPolicy(cql_policy, env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "det_cql_collector = Collector(cql_policy, envs)\n",
    "cql_result = cql_collector.collect(n_episode=40)\n",
    "cql_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/gymnasium/envs/registration.py:513: DeprecationWarning: \u001b[33mWARN: The environment Hopper-v2 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
      "  logger.deprecation(\n",
      "/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/gymnasium/envs/mujoco/mujoco_env.py:211: DeprecationWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n",
      "  logger.deprecation(\n",
      "/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/gymnasium/envs/registration.py:513: DeprecationWarning: \u001b[33mWARN: The environment Hopper-v2 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
      "  logger.deprecation(\n",
      "/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/gymnasium/envs/mujoco/mujoco_env.py:211: DeprecationWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n",
      "  logger.deprecation(\n",
      "/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/gymnasium/envs/registration.py:513: DeprecationWarning: \u001b[33mWARN: The environment Hopper-v2 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
      "  logger.deprecation(\n",
      "/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/gymnasium/envs/mujoco/mujoco_env.py:211: DeprecationWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n",
      "  logger.deprecation(\n",
      "/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/gymnasium/envs/registration.py:513: DeprecationWarning: \u001b[33mWARN: The environment Hopper-v2 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
      "  logger.deprecation(\n",
      "/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/gymnasium/envs/mujoco/mujoco_env.py:211: DeprecationWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n",
      "  logger.deprecation(\n",
      "/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/gymnasium/envs/registration.py:513: DeprecationWarning: \u001b[33mWARN: The environment Hopper-v2 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
      "  logger.deprecation(\n",
      "/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/gymnasium/envs/mujoco/mujoco_env.py:211: DeprecationWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n",
      "  logger.deprecation(\n"
     ]
    }
   ],
   "source": [
    "test_envs = SubprocVectorEnv([lambda: gym.make(args.task) for _ in range(5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/gym/envs/mujoco/mujoco_env.py:190: UserWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n",
      "  logger.warn(\n",
      "/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/gym/spaces/box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "load datafile:   5%|▍         | 1/21 [00:00<00:02,  9.40it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load datafile: 100%|██████████| 21/21 [00:01<00:00, 13.28it/s]\n"
     ]
    }
   ],
   "source": [
    "offline_data = load_buffer_d4rl(\"hopper-medium-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = cql_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/user/R901105/dev/my_fork/tianshou/tianshou/data/collector.py:70: UserWarning: Single environment detected, wrap to DummyVectorEnv.\n",
      "  warnings.warn(\"Single environment detected, wrap to DummyVectorEnv.\")\n"
     ]
    }
   ],
   "source": [
    "test_buffer = VectorReplayBuffer(5000, 5)\n",
    "train_collector = Collector(policy, env, offline_data)\n",
    "test_collector = Collector(policy, test_envs, test_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/user/R901105/dev/log/Hopper-v2/SACPolicy/240423-173106\n"
     ]
    }
   ],
   "source": [
    "# log\n",
    "now = datetime.datetime.now().strftime(\"%y%m%d-%H%M%S\")\n",
    "log_name = os.path.join(args.task, policy.__class__.__name__, now)\n",
    "log_path = os.path.join(\"/data/user/R901105/dev/log\", log_name)\n",
    "writer = SummaryWriter(log_path)\n",
    "logger = TensorboardLogger(writer)\n",
    "print(log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_returns(policy, return_cede_ctrl=False):\n",
    "    policy.eval()\n",
    "    returns = []\n",
    "    cede_ctrl = []\n",
    "    for _ in range(5):\n",
    "        obs, info = env.reset()\n",
    "        done = False\n",
    "        cum_reward = 0\n",
    "        actions = []\n",
    "        if return_cede_ctrl:\n",
    "            cctrl_ep = []\n",
    "        while not done:\n",
    "            batch = Batch(obs=torch.from_numpy(np.expand_dims(obs, 0)).to(device), info=info)\n",
    "            with torch.no_grad():\n",
    "                result = policy(batch)\n",
    "            act = result.act.cpu().squeeze().numpy()\n",
    "            act = policy.map_action(act)\n",
    "            if return_cede_ctrl:\n",
    "                cctrl_ep.append(result.policy.cede_ctrl.cpu().squeeze().numpy())\n",
    "            obs, reward, terminated, truncated, info = env.step(act)\n",
    "            actions.append(act)\n",
    "            cum_reward += reward\n",
    "            done = terminated or truncated\n",
    "        returns.append(cum_reward)\n",
    "        if return_cede_ctrl:\n",
    "            cede_ctrl.append(np.array(cctrl_ep).mean())\n",
    "    if return_cede_ctrl:\n",
    "        return np.array(returns),  np.array(cede_ctrl).mean()\n",
    "    return np.array(returns), _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_fn(num_epoch: int, step_idx: int):\n",
    "    returns, cede_ctrl = get_returns(policy, True)\n",
    "    print(returns.mean(), cede_ctrl.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1: 1001it [00:15, 64.63it/s, alpha=0.323, env_step=1000, gradient_step=1000, len=600, loss/actor=-265.235, loss/alpha=-1.450, loss/critic1=10.468, loss/critic2=10.492, n/ep=0, n/st=1, rew=2024.22]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1: test_reward: 3150.884889 ± 0.000000, best_reward: 3150.884889 ± 0.000000 in #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #2: 1001it [00:16, 60.38it/s, alpha=0.290, env_step=2000, gradient_step=2000, len=446, loss/actor=-263.216, loss/alpha=-1.732, loss/critic1=9.273, loss/critic2=9.195, n/ep=0, n/st=1, rew=1515.25]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #2: test_reward: 3051.355611 ± 0.000000, best_reward: 3150.884889 ± 0.000000 in #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #3: 1001it [00:16, 60.82it/s, alpha=0.261, env_step=3000, gradient_step=3000, len=385, loss/actor=-261.482, loss/alpha=-1.961, loss/critic1=6.918, loss/critic2=7.410, n/ep=0, n/st=1, rew=1276.27]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #3: test_reward: 1518.441726 ± 0.000000, best_reward: 3150.884889 ± 0.000000 in #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #4: 1001it [00:17, 58.05it/s, alpha=0.235, env_step=4000, gradient_step=4000, len=775, loss/actor=-261.592, loss/alpha=-2.190, loss/critic1=3.678, loss/critic2=3.964, n/ep=0, n/st=1, rew=2486.17]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #4: test_reward: 1499.868879 ± 0.000000, best_reward: 3150.884889 ± 0.000000 in #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #5: 1001it [00:16, 59.37it/s, alpha=0.213, env_step=5000, gradient_step=5000, len=387, loss/actor=-260.548, loss/alpha=-2.274, loss/critic1=5.278, loss/critic2=5.245, n/ep=0, n/st=1, rew=1285.96]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #5: test_reward: 2326.359527 ± 0.000000, best_reward: 3150.884889 ± 0.000000 in #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #6: 1001it [00:17, 58.79it/s, alpha=0.193, env_step=6000, gradient_step=6000, len=681, loss/actor=-260.769, loss/alpha=-2.299, loss/critic1=21.613, loss/critic2=21.667, n/ep=0, n/st=1, rew=2302.47]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #6: test_reward: 2495.793671 ± 0.000000, best_reward: 3150.884889 ± 0.000000 in #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #7: 1001it [00:16, 62.05it/s, alpha=0.175, env_step=7000, gradient_step=7000, len=446, loss/actor=-260.838, loss/alpha=-2.431, loss/critic1=5.823, loss/critic2=5.704, n/ep=0, n/st=1, rew=1532.01]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #7: test_reward: 2546.746616 ± 0.000000, best_reward: 3150.884889 ± 0.000000 in #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #8: 1001it [00:16, 61.03it/s, alpha=0.159, env_step=8000, gradient_step=8000, len=618, loss/actor=-260.367, loss/alpha=-2.329, loss/critic1=3.565, loss/critic2=3.313, n/ep=0, n/st=1, rew=2021.87]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #8: test_reward: 1734.818212 ± 0.000000, best_reward: 3150.884889 ± 0.000000 in #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #9: 1001it [00:16, 62.16it/s, alpha=0.145, env_step=9000, gradient_step=9000, len=530, loss/actor=-260.913, loss/alpha=-2.214, loss/critic1=5.988, loss/critic2=5.375, n/ep=0, n/st=1, rew=1816.44]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #9: test_reward: 2299.305655 ± 0.000000, best_reward: 3150.884889 ± 0.000000 in #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #10: 1001it [00:16, 61.41it/s, alpha=0.132, env_step=10000, gradient_step=10000, len=997, loss/actor=-262.608, loss/alpha=-2.144, loss/critic1=5.283, loss/critic2=5.294, n/ep=0, n/st=1, rew=3259.78]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #10: test_reward: 1161.159407 ± 0.000000, best_reward: 3150.884889 ± 0.000000 in #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #11: 1001it [00:16, 61.40it/s, alpha=0.120, env_step=11000, gradient_step=11000, len=768, loss/actor=-263.538, loss/alpha=-2.344, loss/critic1=8.242, loss/critic2=8.235, n/ep=0, n/st=1, rew=2524.81]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #11: test_reward: 1806.836718 ± 0.000000, best_reward: 3150.884889 ± 0.000000 in #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #12: 1001it [00:15, 63.55it/s, alpha=0.109, env_step=12000, gradient_step=12000, len=1000, loss/actor=-263.720, loss/alpha=-2.589, loss/critic1=5.388, loss/critic2=5.822, n/ep=0, n/st=1, rew=3220.69]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #12: test_reward: 1446.258235 ± 0.000000, best_reward: 3150.884889 ± 0.000000 in #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #13: 1001it [00:16, 60.22it/s, alpha=0.099, env_step=13000, gradient_step=13000, len=1000, loss/actor=-265.382, loss/alpha=-2.351, loss/critic1=5.158, loss/critic2=5.312, n/ep=0, n/st=1, rew=3149.43]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #13: test_reward: 3276.466343 ± 0.000000, best_reward: 3276.466343 ± 0.000000 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #14: 1001it [00:15, 63.11it/s, alpha=0.090, env_step=14000, gradient_step=14000, len=410, loss/actor=-264.827, loss/alpha=-2.530, loss/critic1=1.879, loss/critic2=1.895, n/ep=0, n/st=1, rew=1358.36]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #14: test_reward: 3140.961959 ± 0.000000, best_reward: 3276.466343 ± 0.000000 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #15: 1001it [00:15, 62.66it/s, alpha=0.082, env_step=15000, gradient_step=15000, len=361, loss/actor=-266.699, loss/alpha=-2.645, loss/critic1=9.413, loss/critic2=9.524, n/ep=0, n/st=1, rew=1182.79]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #15: test_reward: 3100.963179 ± 0.000000, best_reward: 3276.466343 ± 0.000000 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #16: 1001it [00:16, 61.56it/s, alpha=0.075, env_step=16000, gradient_step=16000, len=1000, loss/actor=-267.484, loss/alpha=-1.935, loss/critic1=1.520, loss/critic2=1.470, n/ep=0, n/st=1, rew=3135.93]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #16: test_reward: 1539.631426 ± 0.000000, best_reward: 3276.466343 ± 0.000000 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #17: 1001it [00:16, 59.73it/s, alpha=0.069, env_step=17000, gradient_step=17000, len=1000, loss/actor=-269.032, loss/alpha=-2.282, loss/critic1=5.119, loss/critic2=5.255, n/ep=0, n/st=1, rew=3147.25]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #17: test_reward: 2739.514495 ± 0.000000, best_reward: 3276.466343 ± 0.000000 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #18: 1001it [00:16, 59.78it/s, alpha=0.062, env_step=18000, gradient_step=18000, len=1000, loss/actor=-268.904, loss/alpha=-2.583, loss/critic1=11.680, loss/critic2=11.497, n/ep=0, n/st=1, rew=3182.67]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #18: test_reward: 1220.447837 ± 0.000000, best_reward: 3276.466343 ± 0.000000 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #19: 1001it [00:16, 60.23it/s, alpha=0.057, env_step=19000, gradient_step=19000, len=930, loss/actor=-269.363, loss/alpha=-1.911, loss/critic1=12.477, loss/critic2=12.331, n/ep=0, n/st=1, rew=3030.99]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #19: test_reward: 1277.539697 ± 0.000000, best_reward: 3276.466343 ± 0.000000 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #20: 1001it [00:15, 63.06it/s, alpha=0.052, env_step=20000, gradient_step=20000, len=588, loss/actor=-270.639, loss/alpha=-1.752, loss/critic1=3.419, loss/critic2=3.401, n/ep=0, n/st=1, rew=1941.56]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #20: test_reward: 1833.551187 ± 0.000000, best_reward: 3276.466343 ± 0.000000 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #21: 1001it [00:15, 64.38it/s, alpha=0.049, env_step=21000, gradient_step=21000, len=831, loss/actor=-270.939, loss/alpha=-1.299, loss/critic1=1.417, loss/critic2=1.309, n/ep=0, n/st=1, rew=2706.63]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #21: test_reward: 2602.023109 ± 0.000000, best_reward: 3276.466343 ± 0.000000 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #22: 1001it [00:15, 66.55it/s, alpha=0.046, env_step=22000, gradient_step=22000, len=919, loss/actor=-272.129, loss/alpha=-1.268, loss/critic1=4.182, loss/critic2=4.167, n/ep=0, n/st=1, rew=2965.25]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #22: test_reward: 2230.951899 ± 0.000000, best_reward: 3276.466343 ± 0.000000 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #23: 1001it [00:15, 66.32it/s, alpha=0.043, env_step=23000, gradient_step=23000, len=864, loss/actor=-273.992, loss/alpha=-0.781, loss/critic1=2.044, loss/critic2=2.173, n/ep=0, n/st=1, rew=2821.43]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #23: test_reward: 1220.884828 ± 0.000000, best_reward: 3276.466343 ± 0.000000 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #24: 1001it [00:15, 63.65it/s, alpha=0.041, env_step=24000, gradient_step=24000, len=461, loss/actor=-274.127, loss/alpha=-1.015, loss/critic1=8.767, loss/critic2=8.626, n/ep=0, n/st=1, rew=1554.69]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #24: test_reward: 1607.255830 ± 0.000000, best_reward: 3276.466343 ± 0.000000 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #25: 1001it [00:15, 63.18it/s, alpha=0.039, env_step=25000, gradient_step=25000, len=463, loss/actor=-275.253, loss/alpha=-0.309, loss/critic1=7.653, loss/critic2=7.392, n/ep=0, n/st=1, rew=1541.84]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #25: test_reward: 2076.696815 ± 0.000000, best_reward: 3276.466343 ± 0.000000 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #26: 1001it [00:15, 65.50it/s, alpha=0.040, env_step=26000, gradient_step=26000, len=550, loss/actor=-276.063, loss/alpha=-0.181, loss/critic1=1.629, loss/critic2=1.674, n/ep=0, n/st=1, rew=1798.26]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #26: test_reward: 3138.571473 ± 0.000000, best_reward: 3276.466343 ± 0.000000 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #27: 1001it [00:15, 65.43it/s, alpha=0.040, env_step=27000, gradient_step=27000, len=347, loss/actor=-278.404, loss/alpha=-0.227, loss/critic1=5.389, loss/critic2=5.553, n/ep=0, n/st=1, rew=1125.99]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #27: test_reward: 1900.467066 ± 0.000000, best_reward: 3276.466343 ± 0.000000 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #28: 1001it [00:16, 61.36it/s, alpha=0.039, env_step=28000, gradient_step=28000, len=459, loss/actor=-277.933, loss/alpha=0.589, loss/critic1=9.420, loss/critic2=9.604, n/ep=0, n/st=1, rew=1477.49]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #28: test_reward: 2538.168791 ± 0.000000, best_reward: 3276.466343 ± 0.000000 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #29: 1001it [00:15, 63.04it/s, alpha=0.040, env_step=29000, gradient_step=29000, len=776, loss/actor=-279.163, loss/alpha=0.595, loss/critic1=8.454, loss/critic2=8.649, n/ep=0, n/st=1, rew=2541.16]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #29: test_reward: 2287.907156 ± 0.000000, best_reward: 3276.466343 ± 0.000000 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #30: 1001it [00:15, 64.54it/s, alpha=0.040, env_step=30000, gradient_step=30000, len=381, loss/actor=-280.233, loss/alpha=-0.310, loss/critic1=4.483, loss/critic2=3.924, n/ep=0, n/st=1, rew=1238.03]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #30: test_reward: 3142.463294 ± 0.000000, best_reward: 3276.466343 ± 0.000000 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #31: 1001it [00:15, 65.34it/s, alpha=0.039, env_step=31000, gradient_step=31000, len=432, loss/actor=-281.350, loss/alpha=0.027, loss/critic1=1.762, loss/critic2=1.914, n/ep=0, n/st=1, rew=1453.10]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #31: test_reward: 1744.012036 ± 0.000000, best_reward: 3276.466343 ± 0.000000 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #32: 1001it [00:15, 64.66it/s, alpha=0.039, env_step=32000, gradient_step=32000, len=993, loss/actor=-283.001, loss/alpha=-0.431, loss/critic1=1.418, loss/critic2=1.449, n/ep=0, n/st=1, rew=3345.54]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #32: test_reward: 2586.593740 ± 0.000000, best_reward: 3276.466343 ± 0.000000 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #33: 1001it [00:15, 64.17it/s, alpha=0.037, env_step=33000, gradient_step=33000, len=376, loss/actor=-284.346, loss/alpha=0.059, loss/critic1=4.632, loss/critic2=4.319, n/ep=0, n/st=1, rew=1236.57]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #33: test_reward: 3170.288325 ± 0.000000, best_reward: 3276.466343 ± 0.000000 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #34: 1001it [00:15, 64.70it/s, alpha=0.036, env_step=34000, gradient_step=34000, len=595, loss/actor=-285.292, loss/alpha=-0.328, loss/critic1=5.017, loss/critic2=4.999, n/ep=0, n/st=1, rew=1979.82]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #34: test_reward: 1751.466476 ± 0.000000, best_reward: 3276.466343 ± 0.000000 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #35: 1001it [00:15, 66.32it/s, alpha=0.035, env_step=35000, gradient_step=35000, len=548, loss/actor=-286.268, loss/alpha=0.137, loss/critic1=3.776, loss/critic2=3.723, n/ep=0, n/st=1, rew=1811.49]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #35: test_reward: 1718.780809 ± 0.000000, best_reward: 3276.466343 ± 0.000000 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #36: 1001it [00:15, 66.34it/s, alpha=0.034, env_step=36000, gradient_step=36000, len=909, loss/actor=-287.211, loss/alpha=-0.734, loss/critic1=2.293, loss/critic2=2.090, n/ep=0, n/st=1, rew=2933.61]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #36: test_reward: 3174.878485 ± 0.000000, best_reward: 3276.466343 ± 0.000000 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #37: 1001it [00:14, 66.81it/s, alpha=0.033, env_step=37000, gradient_step=37000, len=789, loss/actor=-287.222, loss/alpha=-0.174, loss/critic1=7.086, loss/critic2=7.481, n/ep=0, n/st=1, rew=2632.12]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #37: test_reward: 2871.473399 ± 0.000000, best_reward: 3276.466343 ± 0.000000 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #38: 1001it [00:14, 66.99it/s, alpha=0.035, env_step=38000, gradient_step=38000, len=1000, loss/actor=-288.902, loss/alpha=0.227, loss/critic1=11.797, loss/critic2=12.088, n/ep=0, n/st=1, rew=3184.52]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #38: test_reward: 2772.557676 ± 0.000000, best_reward: 3276.466343 ± 0.000000 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #39: 1001it [00:14, 66.74it/s, alpha=0.035, env_step=39000, gradient_step=39000, len=1000, loss/actor=-289.987, loss/alpha=-0.130, loss/critic1=2.567, loss/critic2=2.332, n/ep=0, n/st=1, rew=3266.34]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #39: test_reward: 1740.056198 ± 0.000000, best_reward: 3276.466343 ± 0.000000 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #40: 1001it [00:15, 66.72it/s, alpha=0.035, env_step=40000, gradient_step=40000, len=1000, loss/actor=-290.705, loss/alpha=-0.285, loss/critic1=8.695, loss/critic2=8.970, n/ep=0, n/st=1, rew=3210.67]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #40: test_reward: 1157.252634 ± 0.000000, best_reward: 3276.466343 ± 0.000000 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #41: 1001it [00:15, 66.14it/s, alpha=0.035, env_step=41000, gradient_step=41000, len=455, loss/actor=-290.301, loss/alpha=0.194, loss/critic1=8.675, loss/critic2=8.738, n/ep=0, n/st=1, rew=1480.87]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #41: test_reward: 3163.215912 ± 0.000000, best_reward: 3276.466343 ± 0.000000 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #42: 1001it [00:16, 62.11it/s, alpha=0.035, env_step=42000, gradient_step=42000, len=761, loss/actor=-292.333, loss/alpha=-0.522, loss/critic1=7.311, loss/critic2=7.572, n/ep=0, n/st=1, rew=2546.56]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #42: test_reward: 1485.533674 ± 0.000000, best_reward: 3276.466343 ± 0.000000 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #43: 1001it [00:16, 62.52it/s, alpha=0.034, env_step=43000, gradient_step=43000, len=1000, loss/actor=-293.330, loss/alpha=-0.081, loss/critic1=10.400, loss/critic2=10.616, n/ep=0, n/st=1, rew=3149.38]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #43: test_reward: 1184.983791 ± 0.000000, best_reward: 3276.466343 ± 0.000000 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #44: 1001it [00:15, 65.46it/s, alpha=0.034, env_step=44000, gradient_step=44000, len=858, loss/actor=-294.451, loss/alpha=-0.810, loss/critic1=16.842, loss/critic2=16.938, n/ep=0, n/st=1, rew=2841.65]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #44: test_reward: 843.006448 ± 0.000000, best_reward: 3276.466343 ± 0.000000 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #45: 1001it [00:15, 64.59it/s, alpha=0.033, env_step=45000, gradient_step=45000, len=538, loss/actor=-295.028, loss/alpha=0.227, loss/critic1=6.220, loss/critic2=6.106, n/ep=0, n/st=1, rew=1765.82]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #45: test_reward: 1629.954542 ± 0.000000, best_reward: 3276.466343 ± 0.000000 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #46: 1001it [00:16, 61.16it/s, alpha=0.032, env_step=46000, gradient_step=46000, len=778, loss/actor=-295.680, loss/alpha=0.245, loss/critic1=10.817, loss/critic2=10.645, n/ep=0, n/st=1, rew=2567.66]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #46: test_reward: 1742.104824 ± 0.000000, best_reward: 3276.466343 ± 0.000000 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #47: 1001it [00:15, 63.99it/s, alpha=0.033, env_step=47000, gradient_step=47000, len=442, loss/actor=-295.959, loss/alpha=0.313, loss/critic1=1.677, loss/critic2=1.853, n/ep=0, n/st=1, rew=1422.36]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #47: test_reward: 2825.639337 ± 0.000000, best_reward: 3276.466343 ± 0.000000 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #48: 1001it [00:15, 63.45it/s, alpha=0.035, env_step=48000, gradient_step=48000, len=616, loss/actor=-297.297, loss/alpha=0.359, loss/critic1=8.929, loss/critic2=9.056, n/ep=0, n/st=1, rew=2052.56]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #48: test_reward: 1410.756811 ± 0.000000, best_reward: 3276.466343 ± 0.000000 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #49: 1001it [00:15, 65.19it/s, alpha=0.037, env_step=49000, gradient_step=49000, len=621, loss/actor=-298.506, loss/alpha=0.618, loss/critic1=7.272, loss/critic2=7.432, n/ep=0, n/st=1, rew=2079.96]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #49: test_reward: 1217.864838 ± 0.000000, best_reward: 3276.466343 ± 0.000000 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #50: 1001it [00:15, 65.43it/s, alpha=0.037, env_step=50000, gradient_step=50000, len=458, loss/actor=-299.394, loss/alpha=-0.546, loss/critic1=7.359, loss/critic2=7.405, n/ep=0, n/st=1, rew=1470.23]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #50: test_reward: 1256.330944 ± 0.000000, best_reward: 3276.466343 ± 0.000000 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #51: 1001it [00:15, 65.30it/s, alpha=0.036, env_step=51000, gradient_step=51000, len=608, loss/actor=-301.301, loss/alpha=-0.172, loss/critic1=5.940, loss/critic2=5.773, n/ep=0, n/st=1, rew=2045.17]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #51: test_reward: 1739.912807 ± 0.000000, best_reward: 3276.466343 ± 0.000000 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #52: 1001it [00:15, 64.94it/s, alpha=0.036, env_step=52000, gradient_step=52000, len=383, loss/actor=-302.499, loss/alpha=-0.137, loss/critic1=5.886, loss/critic2=6.323, n/ep=0, n/st=1, rew=1247.30]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #52: test_reward: 3255.556129 ± 0.000000, best_reward: 3276.466343 ± 0.000000 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #53: 1001it [00:15, 63.81it/s, alpha=0.035, env_step=53000, gradient_step=53000, len=379, loss/actor=-303.822, loss/alpha=-0.721, loss/critic1=12.893, loss/critic2=12.607, n/ep=0, n/st=1, rew=1243.48]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #53: test_reward: 2444.425899 ± 0.000000, best_reward: 3276.466343 ± 0.000000 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #54:  51%|#####1    | 514/1000 [00:08<00:08, 58.48it/s, alpha=0.034, env_step=53513, gradient_step=53513, len=1000, loss/actor=-303.479, loss/alpha=-0.610, loss/critic1=2.223, loss/critic2=2.781, n/ep=0, n/st=1, rew=3216.97] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 15\u001b[0m\n\u001b[1;32m      1\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mOffpolicyTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_collector\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_collector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_collector\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_collector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffline_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# test_fn=test_fn,\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstep_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstep_per_collect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepisode_per_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mupdate_per_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_in_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m---> 15\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/user/R901105/dev/my_fork/tianshou/tianshou/trainer/base.py:444\u001b[0m, in \u001b[0;36mBaseTrainer.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_run \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 444\u001b[0m     \u001b[43mdeque\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxlen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# feed the entire iterator into a zero-length deque\u001b[39;00m\n\u001b[1;32m    445\u001b[0m     info \u001b[38;5;241m=\u001b[39m gather_info(\n\u001b[1;32m    446\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_time, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_collector, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_collector,\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_reward, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_reward_std\n\u001b[1;32m    448\u001b[0m     )\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m/data/user/R901105/dev/my_fork/tianshou/tianshou/trainer/base.py:302\u001b[0m, in \u001b[0;36mBaseTrainer.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    299\u001b[0m         result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn/st\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_step)\n\u001b[1;32m    300\u001b[0m         t\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m--> 302\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_update_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m     t\u001b[38;5;241m.\u001b[39mset_postfix(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdata)\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mn \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_fn_flag:\n",
      "File \u001b[0;32m/data/user/R901105/dev/my_fork/tianshou/tianshou/trainer/base.py:511\u001b[0m, in \u001b[0;36mOffpolicyTrainer.policy_update_fn\u001b[0;34m(self, data, result)\u001b[0m\n\u001b[1;32m    509\u001b[0m num_updates \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_per_step \u001b[38;5;241m*\u001b[39m n_collected_steps)\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_updates):\n\u001b[0;32m--> 511\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample_and_update\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_collector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/user/R901105/dev/my_fork/tianshou/tianshou/trainer/base.py:459\u001b[0m, in \u001b[0;36mBaseTrainer._sample_and_update\u001b[0;34m(self, buffer, data)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;66;03m# Note: since sample_size=batch_size, this will perform\u001b[39;00m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;66;03m# exactly one gradient step. This is why we don't need to calculate the\u001b[39;00m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;66;03m# number of gradient steps, like in the on-policy case.\u001b[39;00m\n\u001b[0;32m--> 459\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    460\u001b[0m data\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgradient_step\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_step)})\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_update_data(data, losses)\n",
      "File \u001b[0;32m/data/user/R901105/dev/my_fork/tianshou/tianshou/policy/base.py:346\u001b[0m, in \u001b[0;36mBasePolicy.update\u001b[0;34m(self, sample_size, buffer, **kwargs)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    345\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_fn(batch, buffer, indices)\n\u001b[0;32m--> 346\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_process_fn(batch, buffer, indices)\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/data/user/R901105/dev/my_fork/tianshou/tianshou/policy/modelfree/sac.py:158\u001b[0m, in \u001b[0;36mSACPolicy.learn\u001b[0;34m(self, batch, *args, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m batch\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m (td1 \u001b[38;5;241m+\u001b[39m td2) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2.0\u001b[39m  \u001b[38;5;66;03m# prio-buffer\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;66;03m# actor\u001b[39;00m\n\u001b[0;32m--> 158\u001b[0m obs_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m act \u001b[38;5;241m=\u001b[39m obs_result\u001b[38;5;241m.\u001b[39mact\n\u001b[1;32m    160\u001b[0m current_q1a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic1(batch\u001b[38;5;241m.\u001b[39mobs, act)\u001b[38;5;241m.\u001b[39mflatten()\n",
      "File \u001b[0;32m/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data/user/R901105/dev/my_fork/tianshou/tianshou/policy/modelfree/sac.py:117\u001b[0m, in \u001b[0;36mSACPolicy.forward\u001b[0;34m(self, batch, state, input, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    111\u001b[0m     batch: RolloutBatchProtocol,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    115\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DistLogProbBatchProtocol:\n\u001b[1;32m    116\u001b[0m     obs \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;28minput\u001b[39m]\n\u001b[0;32m--> 117\u001b[0m     logits, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(logits, \u001b[38;5;28mtuple\u001b[39m)\n\u001b[1;32m    119\u001b[0m     dist \u001b[38;5;241m=\u001b[39m Independent(Normal(\u001b[38;5;241m*\u001b[39mlogits), \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data/user/R901105/dev/my_fork/tianshou/tianshou/utils/net/continuous.py:211\u001b[0m, in \u001b[0;36mActorProb.forward\u001b[0;34m(self, obs, state, info)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    206\u001b[0m     obs: Union[np\u001b[38;5;241m.\u001b[39mndarray, torch\u001b[38;5;241m.\u001b[39mTensor],\n\u001b[1;32m    207\u001b[0m     state: Any \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    208\u001b[0m     info: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m {},\n\u001b[1;32m    209\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor], Any]:\n\u001b[1;32m    210\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Mapping: obs -> logits -> (mu, sigma).\"\"\"\u001b[39;00m\n\u001b[0;32m--> 211\u001b[0m     logits, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m     mu \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmu(logits)\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unbounded:\n",
      "File \u001b[0;32m/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data/user/R901105/dev/my_fork/tianshou/tianshou/utils/net/common.py:279\u001b[0m, in \u001b[0;36mNet.forward\u001b[0;34m(self, obs, state, **kwargs)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    269\u001b[0m     obs: Union[np\u001b[38;5;241m.\u001b[39mndarray, torch\u001b[38;5;241m.\u001b[39mTensor],\n\u001b[1;32m    270\u001b[0m     state: Any \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    272\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, Any]:\n\u001b[1;32m    273\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Mapping: obs -> flatten (inside MLP)-> logits.\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m    :param obs:\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;124;03m    :param state: unused and returned as is\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;124;03m    :param kwargs: unused\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 279\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_dueling:  \u001b[38;5;66;03m# Dueling DQN\u001b[39;00m\n",
      "File \u001b[0;32m/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data/user/R901105/dev/my_fork/tianshou/tianshou/utils/net/common.py:150\u001b[0m, in \u001b[0;36mMLP.forward\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflatten_input:\n\u001b[1;32m    149\u001b[0m     obs \u001b[38;5;241m=\u001b[39m obs\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/torch/nn/modules/container.py:216\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/torch/nn/modules/container.py:209\u001b[0m, in \u001b[0;36mSequential.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;129m@_copy_to_script_wrapper\u001b[39m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Module]:\n\u001b[0;32m--> 209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28miter\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mvalues())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "result = OffpolicyTrainer(\n",
    "    policy=policy,\n",
    "    train_collector=train_collector,\n",
    "    test_collector=test_collector,\n",
    "    buffer=offline_data,\n",
    "    # test_fn=test_fn,\n",
    "    max_epoch=200,\n",
    "    step_per_epoch=1000,\n",
    "    step_per_collect=1,\n",
    "    episode_per_test=1,\n",
    "    batch_size=256,\n",
    "    logger=logger,\n",
    "    update_per_step=1,\n",
    "    test_in_train=False,\n",
    ").run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
