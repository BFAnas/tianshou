{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from torch.distributions import Bernoulli\n",
    "import torch\n",
    "import numpy as np\n",
    "from examples.offline.utils import load_buffer_d4rl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "__all__ = ['RewardHighVelocity',\n",
    "           'RewardUnhealthyPose',\n",
    "           'RewardScale']\n",
    "\n",
    "\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "class RewardHighVelocity(gym.RewardWrapper):\n",
    "    \"\"\"Wrapper to modify environment rewards of 'Cheetah','Walker' and\n",
    "    'Hopper'.\n",
    "\n",
    "    Penalizes with certain probability if velocity of the agent is greater\n",
    "    than a predefined max velocity.\n",
    "    Parameters\n",
    "    ----------\n",
    "    kwargs: dict\n",
    "    with keys:\n",
    "    'prob_vel_penal': prob of penalization\n",
    "    'cost_vel': cost of penalization\n",
    "    'max_vel': max velocity\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    step(action): next_state, reward, done, info\n",
    "    execute a step in the environment.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env, **kwargs):\n",
    "        super(RewardHighVelocity, self).__init__(env)\n",
    "        self.penal_v_distr = Bernoulli(kwargs['prob_vel_penal'])\n",
    "        self.penal = kwargs['cost_vel']\n",
    "        self.max_vel = kwargs['max_vel']\n",
    "        self.max_step = kwargs['max_step']\n",
    "        self.step_counter = 0\n",
    "        allowed_envs = ['Cheetah', 'Hopper', 'Walker']\n",
    "        assert(any(e in self.env.unwrapped.spec.id for e in allowed_envs)), \\\n",
    "            'Env {self.env.unwrapped.spec.id} not allowed for RewardWrapper'\n",
    "\n",
    "    def step(self, action):\n",
    "        observation, reward, terminated, truncated, info = self.env.step(action)\n",
    "        vel = self.env.sim.data.qvel[0]\n",
    "        info['risky_state'] = vel > self.max_vel\n",
    "        info['angle'] = self.env.sim.data.qpos[2]\n",
    "        self.step_counter += 1\n",
    "\n",
    "        if self.step_counter > self.max_step:\n",
    "            truncated = True\n",
    "\n",
    "        if 'Cheetah' in self.env.unwrapped.spec.id:\n",
    "            return (observation, self.new_reward(reward, info),\n",
    "                     terminated, truncated, info)\n",
    "        if 'Walker' in self.env.unwrapped.spec.id:\n",
    "            return (observation, self.new_reward(reward, info),\n",
    "                     terminated, truncated, info)\n",
    "        if 'Hopper' in self.env.unwrapped.spec.id:\n",
    "            return (observation, self.new_reward(reward, info),\n",
    "                     terminated, truncated, info)\n",
    "\n",
    "    def new_reward(self, reward, info):\n",
    "        if 'Cheetah' in self.env.unwrapped.spec.id:\n",
    "            forward_reward = info['reward_run']\n",
    "        else:\n",
    "            forward_reward = info['x_velocity']\n",
    "\n",
    "        penal = info['risky_state'] * \\\n",
    "            self.penal_v_distr.sample().item() * self.penal\n",
    "\n",
    "        # If penalty applied, substract the forward_reward from total_reward\n",
    "        # original_reward = rew_healthy + forward_reward - cntrl_cost\n",
    "        new_reward = penal + reward + (penal != 0) * (-forward_reward)\n",
    "        return new_reward\n",
    "    \n",
    "    def reset(self, *, seed: int | None = None, options: dict[str, Any] | None = None) -> tuple[Any, dict[str, Any]]:\n",
    "        self.step_counter = 0\n",
    "        return super().reset(seed=seed, options=options)\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return f'{self.__class__.__name__}{self.env}'\n",
    "\n",
    "\n",
    "class RewardUnhealthyPose(gym.RewardWrapper):\n",
    "    \"\"\"Wrapper to modify environment rewards of 'Walker' and 'Hopper'.\n",
    "    Penalizes with certain probability if pose of the agent doesn't lie\n",
    "    in a 'robust' state space.\n",
    "    Parameters\n",
    "    ----------\n",
    "    kwargs: dict\n",
    "    with keys:\n",
    "    'prob_pose_penal': prob of penalization\n",
    "    'cost_pose': cost of penalization\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    step(action): next_state, reward, done, info\n",
    "    execute a step in the environment.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env, **kwargs):\n",
    "\n",
    "        super(RewardUnhealthyPose, self).__init__(env)\n",
    "\n",
    "        self.penal_distr = Bernoulli(kwargs['prob_pose_penal'])\n",
    "        self.penal = kwargs['cost_pose']\n",
    "        if 'Walker' in self.env.unwrapped.spec.id:\n",
    "            self.robust_angle_range = (-0.5, 0.5)\n",
    "            self.healthy_angle_range = (-1, 1)  # default env\n",
    "\n",
    "        elif 'Hopper' in self.env.unwrapped.spec.id:\n",
    "            self.robust_angle_range = (-0.1, 0.1)\n",
    "            self.healthy_angle_range = (-0.2, 0.2)  # default env\n",
    "\n",
    "        else:\n",
    "            raise ValueError('Environment is not Walker neither Hopper '\n",
    "                             f'for {self.__class__.__name__}')\n",
    "\n",
    "    @property\n",
    "    def is_robust_healthy(self):\n",
    "        z, angle = self.env.sim.data.qpos[1:3]\n",
    "        min_angle, max_angle = self.robust_angle_range\n",
    "        robust_angle = min_angle < angle < max_angle\n",
    "        is_robust_healthy = robust_angle  # and healthy_z\n",
    "        return is_robust_healthy\n",
    "\n",
    "    @property\n",
    "    def is_healthy(self):\n",
    "        z, angle = self.env.sim.data.qpos[1:3]\n",
    "        h_min_angle, h_max_angle = self.healthy_angle_range\n",
    "        healthy_angle = h_min_angle < angle < h_max_angle\n",
    "        self.is_healthy = healthy_angle\n",
    "\n",
    "    def step(self, action):\n",
    "        observation, reward, done, info = self.env.step(action)\n",
    "        info['risky_state'] = ~self.is_robust_healthy\n",
    "        info['angle'] = self.env.sim.data.qpos[2]\n",
    "        return observation, self.new_reward(reward), done, info\n",
    "\n",
    "    def new_reward(self, reward):\n",
    "        # Compute new reward according to penalty probability and agent state:\n",
    "\n",
    "        # Penalty occurs if agent's pose is not robust with certain prob\n",
    "        # If env.terminate when unhealthy=False (i.e. episode doesn't finish\n",
    "        # when unhealthy pose), we do not add penalization when not in\n",
    "        # healty pose.\n",
    "\n",
    "        penal = (~self.is_robust_healthy) * (self.is_healthy) *\\\n",
    "            self.penal_distr.sample().item() * self.penal\n",
    "\n",
    "        new_reward = penal + reward\n",
    "        return new_reward\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return f'{self.__class__.__name__}{self.env}'\n",
    "\n",
    "\n",
    "class RewardScale(gym.RewardWrapper):\n",
    "    def __init__(self, env, scale):\n",
    "\n",
    "        gym.RewardWrapper.__init__(self, env)\n",
    "        self.scale = scale\n",
    "\n",
    "    def reward(self, reward):\n",
    "        return reward * self.scale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"HalfCheetah-v3\"\n",
    "task_data = \"halfcheetah-medium-v0\"\n",
    "prob_vel_penal = 0.05\n",
    "max_vel = 4\n",
    "cost_vel = -70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load datafile: 100%|██████████| 5/5 [00:00<00:00,  7.69it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_buffer_d4rl(task_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/gymnasium/envs/registration.py:513: DeprecationWarning: \u001b[33mWARN: The environment HalfCheetah-v3 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
      "  logger.deprecation(\n",
      "/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/gymnasium/envs/mujoco/mujoco_env.py:211: DeprecationWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n",
      "  logger.deprecation(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stochastic_dataset_halfcheetah(env, dataset):\n",
    "    env = RewardHighVelocity(env, prob_vel_penal=prob_vel_penal, max_vel=max_vel, cost_vel=cost_vel)\n",
    "    done =True\n",
    "    for i in range(len(dataset)):\n",
    "        if done:\n",
    "            env.reset()\n",
    "        env.set_state(qpos=np.concatenate(([i],dataset.obs[i][:8])), qvel=dataset.obs[i][8:])\n",
    "        act, _, done = dataset.act[i], dataset.obs_next[i], dataset.done[i]\n",
    "        _, rew, _, _, _ =  env.step(act)\n",
    "        dataset.rew[i] = rew\n",
    "        \n",
    "    return dataset\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "stochastic_dataset = create_stochastic_dataset_halfcheetah(env, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "stochastic_dataset.save_hdf5(f\"tianshou_buffer_{task_data}_prob{prob_vel_penal}_vel{max_vel}_cost{cost_vel}.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
