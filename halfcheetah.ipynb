{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load offline trained agent\n",
    "# load expert agent\n",
    "# write ensemble class for offline trained agents\n",
    "# use distributional RL to detect risky states\n",
    "# use ensembles to detect novel states\n",
    "# if novelty is above a treshold give control to expert \n",
    "# if risk is above a treshold give control to expert\n",
    "# can conformal prediction give us guaranties about the performance in this setu?p\n",
    "# empirecally verify if we are able to get the desired performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/glfw/__init__.py:916: GLFWError: (65544) b'X11: The DISPLAY environment variable is missing'\n",
      "  warnings.warn(message, GLFWError)\n",
      "/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/gymnasium/envs/registration.py:596: UserWarning: \u001b[33mWARN: plugin: shimmy.registration:register_gymnasium_envs raised Traceback (most recent call last):\n",
      "  File \"/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/gymnasium/envs/registration.py\", line 594, in load_plugin_envs\n",
      "    fn()\n",
      "  File \"/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/shimmy/registration.py\", line 303, in register_gymnasium_envs\n",
      "    _register_dm_control_envs()\n",
      "  File \"/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/shimmy/registration.py\", line 63, in _register_dm_control_envs\n",
      "    from shimmy.dm_control_compatibility import DmControlCompatibilityV0\n",
      "  File \"/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/shimmy/dm_control_compatibility.py\", line 13, in <module>\n",
      "    import dm_env\n",
      "ModuleNotFoundError: No module named 'dm_env'\n",
      "\u001b[0m\n",
      "  logger.warn(f\"plugin: {plugin.value} raised {traceback.format_exc()}\")\n",
      "Warning: Mujoco-based envs failed to import. Set the environment variable D4RL_SUPPRESS_IMPORT_ERROR=1 to suppress this message.\n",
      "No module named 'mjrl'\n",
      "Warning: Flow failed to import. Set the environment variable D4RL_SUPPRESS_IMPORT_ERROR=1 to suppress this message.\n",
      "No module named 'flow'\n",
      "Warning: FrankaKitchen failed to import. Set the environment variable D4RL_SUPPRESS_IMPORT_ERROR=1 to suppress this message.\n",
      "No module named 'dm_env'\n",
      "Warning: CARLA failed to import. Set the environment variable D4RL_SUPPRESS_IMPORT_ERROR=1 to suppress this message.\n",
      "No module named 'carla'\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from types import SimpleNamespace\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from examples.offline.utils import load_buffer_d4rl\n",
    "from tianshou.policy import DSACPolicy, BasePolicy\n",
    "from tianshou.data.buffer.vecbuf import VectorReplayBuffer\n",
    "from tianshou.env import SubprocVectorEnv\n",
    "from tianshou.data import Collector, Batch, to_torch\n",
    "from tianshou.data.types import RolloutBatchProtocol\n",
    "from tianshou.utils.net.common import Net\n",
    "from tianshou.utils.net.continuous import ActorProb, QuantileMlp\n",
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "seed = 0\n",
    "np.random.seed(seed);\n",
    "torch.manual_seed(seed);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_value(value):\n",
    "    # Convert simple types (int, float, bool, None)\n",
    "    if value.isdigit():\n",
    "        return int(value)\n",
    "    elif re.match(r'^\\d+\\.\\d+$', value):\n",
    "        return float(value)\n",
    "    elif value == \"True\":\n",
    "        return True\n",
    "    elif value == \"False\":\n",
    "        return False\n",
    "    elif value == \"None\":\n",
    "        return None\n",
    "    elif value.startswith(\"[\") and value.endswith(\"]\"):\n",
    "        # Convert the list items\n",
    "        items = re.split(r',(?=[^\\]]*(?:\\[|$))', value[1:-1])\n",
    "        return [parse_value(item.strip()) for item in items]\n",
    "    elif value.startswith(\"(\") and value.endswith(\")\"):\n",
    "        # Convert the tuple items\n",
    "        items = re.split(r',(?=[^\\)]*(?:\\(|$))', value[1:-1])\n",
    "        # Special case for single-item tuple\n",
    "        if len(items) == 2 and items[0].strip() != '':\n",
    "            return (parse_value(items[0].strip()),)\n",
    "        return tuple(parse_value(item.strip()) for item in items)\n",
    "    elif value.startswith(\"'\") and value.endswith(\"'\"):\n",
    "        return value[1:-1]\n",
    "    # Else, return the value as-is\n",
    "    return value\n",
    "\n",
    "def get_args(event_file):\n",
    "    ea = EventAccumulator(event_file)\n",
    "    ea.Reload()  # Load the file\n",
    "    # Get the text data\n",
    "    texts = ea.Tags()[\"tensors\"]\n",
    "    # Extract the actual text content\n",
    "    text_data = {}\n",
    "    for tag in texts:\n",
    "        events = ea.Tensors(tag)\n",
    "        for event in events:\n",
    "            # You can extract the wall_time and step if needed\n",
    "            # wall_time, step, value = event.wall_time, event.step, event.text\n",
    "            text_data[tag] = event.tensor_proto.string_val\n",
    "    data = text_data['args/text_summary'][0]\n",
    "    # Convert bytes to string\n",
    "    data_str = data.decode('utf-8')\n",
    "    # Remove the \"Namespace(\" prefix and the trailing \")\"\n",
    "    data_str = data_str[len(\"Namespace(\"):-1]\n",
    "    # Split into key-value pairs\n",
    "    key_values = re.split(r',(?=\\s\\w+=)', data_str)\n",
    "    # Parse each key-value pair\n",
    "    args_dict = {}\n",
    "    for kv in key_values:\n",
    "        key, value = kv.split('=', 1)\n",
    "        key = key.strip()\n",
    "        args_dict[key] = parse_value(value)\n",
    "    args = SimpleNamespace(**args_dict)\n",
    "    try:\n",
    "        env = gym.make(args.task)\n",
    "        target_entropy = -np.prod(env.action_space.shape)\n",
    "        log_alpha = torch.zeros(1, requires_grad=True, device=device)\n",
    "        alpha_optim = torch.optim.Adam([log_alpha], lr=args.alpha_lr)\n",
    "        args.alpha = (target_entropy, log_alpha, alpha_optim)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dsac_args():\n",
    "    args = argparse.Namespace(\n",
    "        task=\"HalfCheetah-v2\",\n",
    "        risk_type=\"wang\",\n",
    "        buffer_size=1000000,\n",
    "        hidden_sizes=[256, 256, 256],\n",
    "        # hidden_sizes=[256, 256],\n",
    "        actor_lr=3e-4,\n",
    "        critic_lr=3e-4,\n",
    "        gamma=0.99,\n",
    "        tau=0.005,\n",
    "        alpha=0.4,\n",
    "        start_timesteps=1,\n",
    "        epoch=200,\n",
    "        step_per_epoch=5000,\n",
    "        step_per_collect=1,\n",
    "        update_per_step=1,\n",
    "        batch_size=256,\n",
    "        training_num=1,\n",
    "        test_num=10,\n",
    "        distortion_param=0.75,\n",
    "    )\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_policy(args, path):\n",
    "    env = gym.make(args.task)\n",
    "    args.state_shape = env.observation_space.shape or env.observation_space.n\n",
    "    args.action_shape = env.action_space.shape or env.action_space.n\n",
    "    # model\n",
    "    net_a = Net(args.state_shape, hidden_sizes=args.hidden_sizes, device=device)\n",
    "    actor = ActorProb(\n",
    "        net_a,\n",
    "        args.action_shape,\n",
    "        device=device,\n",
    "        unbounded=True,\n",
    "        conditioned_sigma=True,\n",
    "    ).to(device)\n",
    "    actor_optim = torch.optim.Adam(actor.parameters(), lr=args.actor_lr)\n",
    "    critic1 = QuantileMlp(hidden_sizes=args.hidden_sizes, input_size=args.state_shape[0] + args.action_shape[0], device=device).to(device)\n",
    "    critic1_optim = torch.optim.Adam(critic1.parameters(), lr=args.critic_lr)\n",
    "    critic2 = QuantileMlp(hidden_sizes=args.hidden_sizes, input_size=args.state_shape[0] + args.action_shape[0], device=device).to(device)\n",
    "    critic2_optim = torch.optim.Adam(critic2.parameters(), lr=args.critic_lr)\n",
    "    policy = DSACPolicy(\n",
    "        actor,\n",
    "        actor_optim,\n",
    "        critic1,\n",
    "        critic1_optim,\n",
    "        critic2,\n",
    "        critic2_optim,\n",
    "        risk_type='wang',\n",
    "        tau=args.tau,\n",
    "        gamma=args.gamma,\n",
    "        alpha=0.4,\n",
    "        action_space=env.action_space,\n",
    "        device=device,\n",
    "        distortion_param=0.75,\n",
    "    )\n",
    "    dirname = os.path.dirname(path)\n",
    "    if os.path.isfile(os.path.join(dirname, \"actor.pth\")):\n",
    "        policy.actor.load_state_dict(torch.load(os.path.join(dirname, \"actor.pth\"), map_location=device))\n",
    "        print(\"Loaded actor from: \", os.path.join(dirname, \"actor.pth\"))\n",
    "    if os.path.isfile(os.path.join(dirname, \"critic1.pth\")):\n",
    "        policy.critic1.load_state_dict(torch.load(os.path.join(dirname, \"critic1.pth\"), map_location=device))\n",
    "        policy.critic1_old.load_state_dict(torch.load(os.path.join(dirname, \"critic1.pth\"), map_location=device))\n",
    "        print(\"Loaded critic1 from: \", os.path.join(dirname, \"critic1.pth\"))\n",
    "    if os.path.isfile(os.path.join(dirname, \"critic2.pth\")):\n",
    "        policy.critic2.load_state_dict(torch.load(os.path.join(dirname, \"critic2.pth\"), map_location=device))\n",
    "        policy.critic2_old.load_state_dict(torch.load(os.path.join(dirname, \"critic2.pth\"), map_location=device))\n",
    "        print(\"Loaded critic2 from: \", os.path.join(dirname, \"critic2.pth\"))\n",
    "    else:\n",
    "        policy.load_state_dict(torch.load(path, map_location=device))\n",
    "        print(\"Loaded agent from: \", path)\n",
    "    return policy\n",
    "\n",
    "def load_behavioral_crtitic(args, path):\n",
    "    behavioral_critic = QuantileMlp(\n",
    "        input_size=args.state_shape[0] + args.action_shape[0],\n",
    "        hidden_sizes=args.hidden_sizes,\n",
    "        device=device,\n",
    "    ).to(device)\n",
    "    behavioral_critic.load_state_dict(torch.load(path, map_location=device))\n",
    "    return behavioral_critic\n",
    "\n",
    "def get_model(log_path, type=None):\n",
    "    if type == \"behavioral\":\n",
    "        files = os.listdir(log_path)\n",
    "        event_file = [f for f in files if f.startswith('event')][0]\n",
    "        full_path = os.path.join(log_path, event_file)\n",
    "        args = get_args(full_path)\n",
    "        resume_path = os.path.join(log_path, 'model.pth')\n",
    "        policy = load_behavioral_crtitic(args, resume_path)\n",
    "    elif type == \"codac\":\n",
    "        files = os.listdir(log_path)\n",
    "        event_file = [f for f in files if f.startswith('event')][0]\n",
    "        full_path = os.path.join(log_path, event_file)\n",
    "        args = get_args(full_path)\n",
    "        resume_path = os.path.join(log_path, 'policy.pth')\n",
    "        policy = load_policy(args, resume_path)\n",
    "    else:\n",
    "        args = get_dsac_args()\n",
    "        resume_path = os.path.join(log_path, 'policy.pth')\n",
    "        policy = load_policy(args, resume_path)\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = \"/data/user/R901105/dev/log/HalfCheetah-v4/qr/231127-112017\"\n",
    "behavioral_critic = get_model(log_path, \"behavioral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/gymnasium/envs/registration.py:513: DeprecationWarning: \u001b[33mWARN: The environment HalfCheetah-v2 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
      "  logger.deprecation(\n",
      "/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/gymnasium/envs/mujoco/mujoco_env.py:211: DeprecationWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n",
      "  logger.deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded actor from:  /data/user/R901105/dev/log/HalfCheetah-v4/codac_bc/neutral/0/231127-113302/actor.pth\n",
      "Loaded critic1 from:  /data/user/R901105/dev/log/HalfCheetah-v4/codac_bc/neutral/0/231127-113302/critic1.pth\n",
      "Loaded critic2 from:  /data/user/R901105/dev/log/HalfCheetah-v4/codac_bc/neutral/0/231127-113302/critic2.pth\n"
     ]
    }
   ],
   "source": [
    "log_path1 = \"/data/user/R901105/dev/log/HalfCheetah-v4/codac_bc/neutral/0/231127-113302\"\n",
    "offline_policy1 = get_model(log_path1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded actor from:  /data/user/R901105/dev/log/HalfCheetah-v4/codac_bc/neutral/123/231127-124113/actor.pth\n",
      "Loaded critic1 from:  /data/user/R901105/dev/log/HalfCheetah-v4/codac_bc/neutral/123/231127-124113/critic1.pth\n",
      "Loaded critic2 from:  /data/user/R901105/dev/log/HalfCheetah-v4/codac_bc/neutral/123/231127-124113/critic2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/gymnasium/envs/registration.py:513: DeprecationWarning: \u001b[33mWARN: The environment HalfCheetah-v2 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
      "  logger.deprecation(\n",
      "/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/gymnasium/envs/mujoco/mujoco_env.py:211: DeprecationWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n",
      "  logger.deprecation(\n"
     ]
    }
   ],
   "source": [
    "log_path2 = \"/data/user/R901105/dev/log/HalfCheetah-v4/codac_bc/neutral/123/231127-124113\"\n",
    "offline_policy2 = get_model(log_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded actor from:  /data/user/R901105/dev/log/HalfCheetah-v4/codac_bc/neutral/0/231204-225919/actor.pth\n",
      "Loaded critic1 from:  /data/user/R901105/dev/log/HalfCheetah-v4/codac_bc/neutral/0/231204-225919/critic1.pth\n",
      "Loaded critic2 from:  /data/user/R901105/dev/log/HalfCheetah-v4/codac_bc/neutral/0/231204-225919/critic2.pth\n"
     ]
    }
   ],
   "source": [
    "log_path3 = \"/data/user/R901105/dev/log/HalfCheetah-v4/codac_bc/neutral/0/231204-225919\"\n",
    "offline_policy3 = get_model(log_path3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded actor from:  /data/user/R901105/dev/halfcheetah_expert/actor.pth\n",
      "Loaded critic1 from:  /data/user/R901105/dev/halfcheetah_expert/critic1.pth\n",
      "Loaded critic2 from:  /data/user/R901105/dev/halfcheetah_expert/critic2.pth\n"
     ]
    }
   ],
   "source": [
    "log_path = \"/data/user/R901105/dev/halfcheetah_expert\"\n",
    "expert_policy = get_model(log_path) \n",
    "expert_policy.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeterministicPolicy(BasePolicy):\n",
    "    def __init__(self, policy, action_space):\n",
    "        super().__init__(action_space=action_space)\n",
    "        self.policy = policy\n",
    "\n",
    "    def train(self, mode: bool = True) -> \"DeterministicPolicy\":\n",
    "        self.policy.eval()\n",
    "        return self\n",
    "    \n",
    "    def forward(self, batch, state=None, **kwargs):\n",
    "        return self.policy(batch)\n",
    "    \n",
    "    def learn(self, batch, **kwargs):\n",
    "        info = self.policy.learn(batch)\n",
    "        return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "class EnsemblePolicy(BasePolicy):\n",
    "    def __init__(self, policies, action_space):\n",
    "        super().__init__(action_space=action_space)\n",
    "        self.policies = policies\n",
    "\n",
    "    def forward(self, batch, state=None, **kwargs):\n",
    "        return self.policies[0](batch)\n",
    "    \n",
    "    def learn(self, batch, **kwargs):\n",
    "        policy = random.choice(self.policies)\n",
    "        info = policy.learn(batch)\n",
    "        # for policy in self.policies:\n",
    "        #     info = policy.learn(batch)\n",
    "        return info\n",
    "    \n",
    "    def bc(self, batch):\n",
    "        policy = random.choice(self.policies)\n",
    "        pred_act = policy.actor(batch.obs)\n",
    "        expert_act = batch.act\n",
    "        loss = (pred_act - expert_act).pow(2).mean()\n",
    "        policy.actor_optim.zero_grad()\n",
    "        loss.backward()\n",
    "        policy.actor_optim.step()\n",
    "        return loss\n",
    "\n",
    "    def get_qvalues(self, obs, act):\n",
    "        q_values = torch.stack([p.critic1(obs, act).detach() for p in self.policies])\n",
    "        return q_values\n",
    "    \n",
    "    def train(self, mode: bool = True) -> \"EnsemblePolicy\":\n",
    "        for policy in self.policies:\n",
    "            policy.train(mode)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"HalfCheetah-v2\"\n",
    "env_num = 20\n",
    "env = gym.make(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "policies = [offline_policy1, offline_policy2, offline_policy3]\n",
    "# policies = [offline_policy1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = EnsemblePolicy(policies, env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/gym/envs/mujoco/mujoco_env.py:190: UserWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n",
      "  logger.warn(\n",
      "/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/gym/spaces/box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "load datafile: 100%|██████████| 21/21 [00:01<00:00, 11.08it/s]\n"
     ]
    }
   ],
   "source": [
    "offline_data = load_buffer_d4rl(\"halfcheetah-medium-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# give control to the expert policy based on epistemic and aleatoric uncertainties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixedPolicy(BasePolicy):\n",
    "    def __init__(self, ensemble: EnsemblePolicy, expert_policy: BasePolicy, old_critic: QuantileMlp, action_space, config=\"nr\"):\n",
    "        super().__init__(action_space=action_space)\n",
    "        self.ensemble = ensemble\n",
    "        self.expert_policy = expert_policy\n",
    "        self.old_critic = old_critic\n",
    "        self.config = config\n",
    "\n",
    "    def update_results(self, batch: RolloutBatchProtocol):\n",
    "        self.ensemble_result = self.ensemble(batch)\n",
    "        self.expert_result = self.expert_policy(batch)\n",
    "        actions = self.ensemble_result.act\n",
    "        expert_actions = self.expert_result.act\n",
    "        self.q_values = self.ensemble.get_qvalues(batch.obs, actions) # (n_policies, bsz, n_taus)\n",
    "        self.expert_q_values = self.expert_policy.critic1(batch.obs, expert_actions)\n",
    "\n",
    "    def update_novelty_threshold(self, buffer, step=0, decay=0.9, start=0.89):\n",
    "        batch, _ = buffer.sample(25000)\n",
    "        batch = to_torch(batch, dtype=torch.float32, device=device)\n",
    "        q_values = self.ensemble.get_qvalues(batch.obs, batch.act)\n",
    "        data_epistemic_uncert = torch.std(q_values, 0).mean(-1).detach().cpu().numpy()\n",
    "        pecentile = 1 - (1 -start) * (decay**step)\n",
    "        self.novelty_threshold = np.quantile(data_epistemic_uncert, pecentile)\n",
    "\n",
    "    def get_uncertainties(self):\n",
    "        epistemic_uncert = torch.std(self.q_values, 0).mean(-1) # bsz\n",
    "        aleatoric_uncert = torch.std(self.q_values, -1).mean(0) # bsz\n",
    "        return epistemic_uncert, aleatoric_uncert\n",
    "\n",
    "    # def cede_control(self):\n",
    "    #     epistemic_uncert, aleatoric_uncert = self.get_uncertainties()\n",
    "    #     novelty = epistemic_uncert > self.novelty_threshold\n",
    "    #     risk = (self.q_values.mean((0, 2)) - aleatoric_uncert) < self.expert_q_values.mean(-1)\n",
    "    #     return novelty, risk\n",
    "\n",
    "    def cede_control(self):\n",
    "        risk = self.q_values.mean((0, 2)) < 0.9*self.expert_q_values.mean(-1)\n",
    "        return None , risk\n",
    "\n",
    "    def take_control(self):\n",
    "        ensemble_actions = self.ensemble_result.act\n",
    "        expert_actions = self.expert_result.act\n",
    "        expert_std = self.expert_result.dist.base_dist.scale\n",
    "        return torch.any(abs(ensemble_actions-expert_actions) < expert_std/2, dim=1)\n",
    "\n",
    "    def forward(self, batch: RolloutBatchProtocol, state=None, **kwargs):\n",
    "        batch = to_torch(batch, dtype=torch.float32, device=device)\n",
    "        self.update_results(batch)\n",
    "        novelty, risk = self.cede_control()\n",
    "        if self.config == \"n\":\n",
    "            cede_ctrl = novelty\n",
    "        elif self.config == \"r\":\n",
    "            cede_ctrl = risk\n",
    "        elif self.config == \"nr\":\n",
    "            cede_ctrl = torch.logical_or(novelty, risk)\n",
    "        else:\n",
    "            cede_ctrl = torch.zeros_like(novelty)\n",
    "        cede_ctrl = cede_ctrl.unsqueeze(-1)\n",
    "        actions = torch.where(cede_ctrl, self.expert_result.act, self.ensemble_result.act)\n",
    "        return Batch(**{'act': actions, 'policy': Batch({'cede_ctrl': cede_ctrl})})\n",
    "\n",
    "    def train(self, mode: bool = True) -> \"MixedPolicy\":\n",
    "        self.ensemble.train(mode)\n",
    "        return self\n",
    "\n",
    "    def learn(self, batch, **kwargs):\n",
    "        # cede_ctrl = batch.policy.cede_ctrl.cpu().squeeze()\n",
    "        # ensemble_batch = batch[~cede_ctrl]\n",
    "        # if len(ensemble_batch) > 0:\n",
    "        #     info = self.ensemble.learn(ensemble_batch)\n",
    "        # else:\n",
    "        #     info = {}\n",
    "        # online_batch = batch\n",
    "        # offline_batch, _ = offline_data.sample(20*len(batch))\n",
    "        # _batch = Batch.cat([online_batch, offline_batch])\n",
    "        info = self.ensemble.learn(batch)\n",
    "        return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixPolicy(BasePolicy):\n",
    "    def __init__(self, base_policy: BasePolicy, expert_policy: BasePolicy, od_detector: nn.Module, action_space):\n",
    "        super().__init__(action_space=action_space)\n",
    "        self.base = base_policy\n",
    "        self.expert = expert_policy\n",
    "        self.od_detector = od_detector\n",
    "\n",
    "    def update_results(self, batch: RolloutBatchProtocol):\n",
    "        self.base_result = self.base(batch)\n",
    "        self.expert_result = self.expert(batch)\n",
    "        self.od_result = self.od_detector(batch.obs)\n",
    "\n",
    "    def cede_control(self):\n",
    "        return 1-self.od_result\n",
    "\n",
    "    def forward(self, batch: RolloutBatchProtocol, state=None, **kwargs):\n",
    "        batch = to_torch(batch, dtype=torch.float32, device=device)\n",
    "        self.update_results(batch)\n",
    "        cede_ctrl = self.cede_control()\n",
    "        actions = torch.where(cede_ctrl, self.expert_result.act, self.ensemble_result.act)\n",
    "        return Batch(**{'act': actions, 'policy': Batch({'cede_ctrl': cede_ctrl})})\n",
    "\n",
    "    def train(self, mode: bool = True) -> \"MixPolicy\":\n",
    "        self.base.eval()\n",
    "        return self\n",
    "\n",
    "    def learn(self, batch, **kwargs):\n",
    "        info = self.base.learn(batch)\n",
    "        return info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# estimate novelty_threshold from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tianshou.data.buffer.base import ReplayBuffer\n",
    "\n",
    "# def add_policy_id(buffer: ReplayBuffer) -> ReplayBuffer:\n",
    "#     data_dict = buffer._meta.__dict__\n",
    "#     new_data_dict = data_dict.copy()\n",
    "#     new_data_dict[\"policy\"] = Batch(**{'id': np.full(buffer.rew.shape, -1)})\n",
    "#     new_batch = Batch(**new_data_dict)\n",
    "#     buffer._meta = new_batch\n",
    "#     return buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# offline_data = add_policy_id(offline_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch, _ = offline_data.sample(10)\n",
    "# batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "offline_batch, _ = offline_data.sample(25000)\n",
    "offline_batch = to_torch(offline_batch, dtype=torch.float32, device=device)\n",
    "q_values = ensemble.get_qvalues(offline_batch.obs, offline_batch.act)\n",
    "data_epistemic_uncert = torch.std(q_values, 0).mean(-1).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_bins = int(np.sqrt(data_epistemic_uncert.size))\n",
    "data_range = (np.min(data_epistemic_uncert), np.max(data_epistemic_uncert))\n",
    "quantile_89 = np.quantile(data_epistemic_uncert, 0.89)\n",
    "quantile_99 = np.quantile(data_epistemic_uncert, 0.99)\n",
    "# Plotting the histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(data_epistemic_uncert, bins=num_bins, range=data_range, color='blue', edgecolor='black', alpha=0.7)\n",
    "# Plot a vertical line for the 89% quantile\n",
    "plt.axvline(quantile_89, color='red', linestyle='dashed', linewidth=2)\n",
    "plt.axvline(quantile_99, color='green', linestyle='dashed', linewidth=2)\n",
    "# Annotate the 89% quantile value on the plot\n",
    "plt.text(quantile_89, plt.ylim()[1]*0.9, f'89th Quantile: {quantile_89:.2f}', color='red')\n",
    "plt.text(quantile_99, plt.ylim()[1]*0.75, f'99th Quantile: {quantile_99:.2f}', color='green')\n",
    "plt.title('Histogram')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Establish baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_policy_nr = MixedPolicy(ensemble, expert_policy, behavioral_critic, env.action_space)\n",
    "mix_policy_n = MixedPolicy(ensemble, expert_policy, behavioral_critic, env.action_space, \"n\")\n",
    "mix_policy = MixedPolicy(ensemble, expert_policy, behavioral_critic, env.action_space, \"\")\n",
    "mix_policy_nr.update_novelty_threshold(offline_data)\n",
    "mix_policy_n.update_novelty_threshold(offline_data)\n",
    "mix_policy.update_novelty_threshold(offline_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = SubprocVectorEnv([lambda: gym.make(task) for _ in range(20)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n/ep': 40,\n",
       " 'n/st': 40000,\n",
       " 'rews': array([4861.25123787, 4587.06964968, 4534.28378431, 4457.82757447,\n",
       "        4503.90128177, 4185.88310027, 4675.28780686, 4233.74953302,\n",
       "        4097.39982837, 4570.62058909, 3427.38962896, 4412.58567319,\n",
       "        4849.58622708, 4818.72679523, 4026.32644639, 4750.54978905,\n",
       "        4577.85029425, 4623.40553575, 4123.86082648, 4580.63307277,\n",
       "        4559.47116807, 4563.45771756, 4765.02012956, 4944.46879158,\n",
       "        4833.58586656, 4527.12571751, 4639.44368305, 4569.56132308,\n",
       "        4088.32253109, 4503.86095053, 4459.76761175, 4573.57119094,\n",
       "        4119.7871919 , 4703.84845701, 3849.20943889, 4617.15584606,\n",
       "        4751.27174055, 4721.87837877, 4853.08278055, 4610.35646229]),\n",
       " 'lens': array([1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000,\n",
       "        1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000,\n",
       "        1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000,\n",
       "        1000, 1000, 1000, 1000, 1000, 1000, 1000]),\n",
       " 'idxs': array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "        17, 18, 19,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13,\n",
       "        14, 15, 16, 17, 18, 19]),\n",
       " 'rew': 4503.810891303193,\n",
       " 'len': 1000.0,\n",
       " 'rew_std': 308.50179607568543,\n",
       " 'len_std': 0.0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op_collector = Collector(offline_policy1, envs)\n",
    "op_collector.collect(n_episode=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n/ep': 40,\n",
       " 'n/st': 40000,\n",
       " 'rews': array([4352.14895172, 4496.61104164, 4542.4159473 , 4509.05784289,\n",
       "        4344.24581596, 4812.278767  , 4890.48329849, 4790.8731387 ,\n",
       "        4323.52308048, 4575.40862159, 4567.62842772, 4729.67602471,\n",
       "        4805.64195757, 4849.8561668 , 4444.10678548, 4895.62640168,\n",
       "        4429.70091472, 4458.75371727, 4555.93538403, 4448.47098945,\n",
       "        4327.10597405, 4664.80252436, 4891.05007501, 4503.21708405,\n",
       "        4780.58581933, 4273.9563884 , 4813.4731181 , 4598.562078  ,\n",
       "        4583.38931156, 4338.73452948, 4477.85819315, 4850.25245577,\n",
       "        4791.83259071, 4678.04347364, 4579.7055238 , 4821.44560657,\n",
       "        4431.30873021, 4661.74918346, 4720.06984182, 3979.88458976]),\n",
       " 'lens': array([1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000,\n",
       "        1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000,\n",
       "        1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000,\n",
       "        1000, 1000, 1000, 1000, 1000, 1000, 1000]),\n",
       " 'idxs': array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "        17, 18, 19,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13,\n",
       "        14, 15, 16, 17, 18, 19]),\n",
       " 'rew': 4589.736759160753,\n",
       " 'len': 1000.0,\n",
       " 'rew_std': 207.53567033478018,\n",
       " 'len_std': 0.0}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mix_buffer = VectorReplayBuffer(20000, 20)\n",
    "mix_buffer_n = VectorReplayBuffer(20000, 20)\n",
    "mix_buffer_nr = VectorReplayBuffer(20000, 20)\n",
    "\n",
    "ens_collector = Collector(ensemble, envs)\n",
    "exp_collector = Collector(expert_policy, envs)\n",
    "mix_collector = Collector(mix_policy, envs, mix_buffer)\n",
    "mix_collector_n = Collector(mix_policy_n, envs, mix_buffer_n)\n",
    "mix_collector_nr = Collector(mix_policy_nr, envs, mix_buffer_nr)\n",
    "\n",
    "ens_result = ens_collector.collect(n_episode=40)\n",
    "# exp_result = exp_collector.collect(n_episode=40)\n",
    "# mix_result = mix_collector.collect(n_episode=40)\n",
    "# mix_result_n = mix_collector_n.collect(n_episode=40)\n",
    "# mix_result_nr = mix_collector_nr.collect(n_episode=40)\n",
    "\n",
    "# ens_baseline = ens_result[\"rew\"]\n",
    "\n",
    "# ens_result[\"rew\"], exp_result[\"rew\"], mix_result[\"rew\"], mix_result_n[\"rew\"], mix_result_nr[\"rew\"]\n",
    "ens_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_buffer.policy.cede_ctrl.cpu().float().mean(), mix_buffer_n.policy.cede_ctrl.cpu().float().mean(), mix_buffer_nr.policy.cede_ctrl.cpu().float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch, _ = mix_buffer_nr.sample(0)\n",
    "batch.policy.cede_ctrl.cpu().float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check the performance of the ensemble + expert in comparaison with only the ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policies(ensemble, expert_policy, behavioral_critic, novelty_threshold, n_episode):\n",
    "    mix_policy = MixedPolicy(ensemble, expert_policy, behavioral_critic, env.action_space, novelty_threshold)\n",
    "    mix_collector = Collector(mix_policy, env)\n",
    "    mix_result = mix_collector.collect(n_episode=n_episode)\n",
    "    improvement = (mix_result['rew'] - ens_baseline) / mix_result['rew']\n",
    "    return mix_result['rew'], mix_policy.expert_calls.mean(), improvement    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_discounted_accumulated_rewards(policy: BasePolicy, env_name: str, gamma: float) -> float:\n",
    "    \"\"\"\n",
    "    Run a Tianshou policy in a given environment and calculate the discounted accumulated rewards.\n",
    "\n",
    "    :param policy: Tianshou policy to be evaluated.\n",
    "    :param env_name: Name of the gym environment.\n",
    "    :param gamma: Discount factor for future rewards.\n",
    "    :return: Discounted accumulated reward.\n",
    "    \"\"\"\n",
    "    # Create the environment\n",
    "    env = gym.make(env_name)\n",
    "    state, _ = env.reset()\n",
    "\n",
    "    done = False\n",
    "    discounted_accumulated_reward = 0\n",
    "    accumulated_reward = 0\n",
    "    discount = 1\n",
    "\n",
    "    # Run the policy in the environment\n",
    "    while not done:\n",
    "        # Predict the action given the current state\n",
    "        action = policy(Batch(**{\"obs\": state[np.newaxis, ...], \"info\": 0})).act.squeeze().detach().cpu()\n",
    "\n",
    "        # Take the action in the environment\n",
    "        next_state, reward, truncated, terminated, _ = env.step(action)\n",
    "\n",
    "        # Update the discounted accumulated reward\n",
    "        discounted_accumulated_reward += reward * discount\n",
    "        accumulated_reward += reward\n",
    "        discount *= gamma\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "        done = truncated or terminated\n",
    "\n",
    "    env.close()\n",
    "    return discounted_accumulated_reward, accumulated_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/gymnasium/envs/registration.py:513: DeprecationWarning: \u001b[33mWARN: The environment Hopper-v2 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
      "  logger.deprecation(\n",
      "/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/gymnasium/envs/mujoco/mujoco_env.py:211: DeprecationWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n",
      "  logger.deprecation(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x11 and 17x256)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcalculate_discounted_accumulated_rewards\u001b[49m\u001b[43m(\u001b[49m\u001b[43moffline_policy1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHopper-v2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.99\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[45], line 22\u001b[0m, in \u001b[0;36mcalculate_discounted_accumulated_rewards\u001b[0;34m(policy, env_name, gamma)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Run the policy in the environment\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# Predict the action given the current state\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBatch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnewaxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minfo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mact\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# Take the action in the environment\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     next_state, reward, truncated, terminated, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data/user/R901105/dev/my_fork/tianshou/tianshou/policy/modelfree/sac.py:117\u001b[0m, in \u001b[0;36mSACPolicy.forward\u001b[0;34m(self, batch, state, input, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    111\u001b[0m     batch: RolloutBatchProtocol,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    115\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DistLogProbBatchProtocol:\n\u001b[1;32m    116\u001b[0m     obs \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;28minput\u001b[39m]\n\u001b[0;32m--> 117\u001b[0m     logits, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(logits, \u001b[38;5;28mtuple\u001b[39m)\n\u001b[1;32m    119\u001b[0m     dist \u001b[38;5;241m=\u001b[39m Independent(Normal(\u001b[38;5;241m*\u001b[39mlogits), \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data/user/R901105/dev/my_fork/tianshou/tianshou/utils/net/continuous.py:211\u001b[0m, in \u001b[0;36mActorProb.forward\u001b[0;34m(self, obs, state, info)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    206\u001b[0m     obs: Union[np\u001b[38;5;241m.\u001b[39mndarray, torch\u001b[38;5;241m.\u001b[39mTensor],\n\u001b[1;32m    207\u001b[0m     state: Any \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    208\u001b[0m     info: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m {},\n\u001b[1;32m    209\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor], Any]:\n\u001b[1;32m    210\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Mapping: obs -> logits -> (mu, sigma).\"\"\"\u001b[39;00m\n\u001b[0;32m--> 211\u001b[0m     logits, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m     mu \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmu(logits)\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unbounded:\n",
      "File \u001b[0;32m/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data/user/R901105/dev/my_fork/tianshou/tianshou/utils/net/common.py:279\u001b[0m, in \u001b[0;36mNet.forward\u001b[0;34m(self, obs, state, **kwargs)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    269\u001b[0m     obs: Union[np\u001b[38;5;241m.\u001b[39mndarray, torch\u001b[38;5;241m.\u001b[39mTensor],\n\u001b[1;32m    270\u001b[0m     state: Any \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    272\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, Any]:\n\u001b[1;32m    273\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Mapping: obs -> flatten (inside MLP)-> logits.\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m    :param obs:\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;124;03m    :param state: unused and returned as is\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;124;03m    :param kwargs: unused\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 279\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_dueling:  \u001b[38;5;66;03m# Dueling DQN\u001b[39;00m\n",
      "File \u001b[0;32m/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data/user/R901105/dev/my_fork/tianshou/tianshou/utils/net/common.py:150\u001b[0m, in \u001b[0;36mMLP.forward\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflatten_input:\n\u001b[1;32m    149\u001b[0m     obs \u001b[38;5;241m=\u001b[39m obs\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x11 and 17x256)"
     ]
    }
   ],
   "source": [
    "calculate_discounted_accumulated_rewards(offline_policy1, \"Hopper-v2\", 0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# perform continual learning and observe the evolution of numbers of calls to the expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this can be tricky as we need to do continual learning for all the policies in the ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.seed to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.seed` for environment variables or `env.get_wrapper_attr('seed')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.seed to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.seed` for environment variables or `env.get_wrapper_attr('seed')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.seed to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.seed` for environment variables or `env.get_wrapper_attr('seed')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.seed to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.seed` for environment variables or `env.get_wrapper_attr('seed')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.seed to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.seed` for environment variables or `env.get_wrapper_attr('seed')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "test_envs = SubprocVectorEnv([lambda: gym.make(task) for _ in range(5)])\n",
    "test_envs.seed(seed);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "det_policy = DeterministicPolicy(offline_policy1, env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixx = MixPolicy(det_policy, expert_policy, env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_policy_r = MixedPolicy(ensemble, expert_policy, behavioral_critic, env.action_space, \"r\")\n",
    "mix_policy_r.update_novelty_threshold(offline_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/user/R901105/dev/my_fork/tianshou/tianshou/data/collector.py:70: UserWarning: Single environment detected, wrap to DummyVectorEnv.\n",
      "  warnings.warn(\"Single environment detected, wrap to DummyVectorEnv.\")\n"
     ]
    }
   ],
   "source": [
    "test_buffer = VectorReplayBuffer(5000, 5)\n",
    "train_collector = Collector(mix_policy_r, env, offline_data)\n",
    "test_collector = Collector(mix_policy_r, test_envs, test_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_collector.collect(n_step=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tianshou.utils import TensorboardLogger\n",
    "# log\n",
    "now = datetime.datetime.now().strftime(\"%y%m%d-%H%M%S\")\n",
    "log_name = os.path.join(task, \"dsac\", \"neutral\", now)\n",
    "log_path = os.path.join(\"../../log\", log_name)\n",
    "writer = SummaryWriter(log_path)\n",
    "logger = TensorboardLogger(writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_fn(num_epoch: int, step_idx: int):\n",
    "    if num_epoch > 0 :\n",
    "        # mix_policy_nr.update_novelty_threshold(offline_data, step=num_epoch)\n",
    "        print(test_buffer.policy.cede_ctrl.cpu().float().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1: 501it [00:18, 27.77it/s, env_step=500, gradient_step=500, len=0, loss/actor=-438.377, loss/critic1=8.656, loss/critic2=8.655, n/ep=0, n/st=1, rew=0.00]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "Epoch #1: test_reward: 8743.714185 ± 38.840428, best_reward: 8780.545532 ± 23.223626 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #2: 501it [00:18, 27.40it/s, env_step=1000, gradient_step=1000, len=1353, loss/actor=-437.117, loss/critic1=8.566, loss/critic2=8.565, n/ep=1, n/st=1, rew=11687.88]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "Epoch #2: test_reward: 8762.076868 ± 60.035671, best_reward: 8780.545532 ± 23.223626 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #3: 501it [00:18, 27.61it/s, env_step=1500, gradient_step=1500, len=1353, loss/actor=-435.715, loss/critic1=8.468, loss/critic2=8.467, n/ep=0, n/st=1, rew=11687.88]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "Epoch #3: test_reward: 8791.711548 ± 65.005616, best_reward: 8791.711548 ± 65.005616 in #3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #4: 501it [00:18, 27.68it/s, env_step=2000, gradient_step=2000, len=1000, loss/actor=-434.379, loss/critic1=8.362, loss/critic2=8.362, n/ep=1, n/st=1, rew=8821.95]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "Epoch #4: test_reward: 8768.750872 ± 25.452285, best_reward: 8791.711548 ± 65.005616 in #3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #5: 501it [00:18, 27.55it/s, env_step=2500, gradient_step=2500, len=1000, loss/actor=-433.559, loss/critic1=8.287, loss/critic2=8.287, n/ep=0, n/st=1, rew=8821.95]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "Epoch #5: test_reward: 8760.169083 ± 33.573139, best_reward: 8791.711548 ± 65.005616 in #3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #6: 501it [00:18, 27.50it/s, env_step=3000, gradient_step=3000, len=1000, loss/actor=-432.679, loss/critic1=8.204, loss/critic2=8.204, n/ep=1, n/st=1, rew=8791.12]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "Epoch #6: test_reward: 8771.193704 ± 54.372225, best_reward: 8791.711548 ± 65.005616 in #3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #7: 501it [00:18, 27.50it/s, env_step=3500, gradient_step=3500, len=1000, loss/actor=-431.260, loss/critic1=8.124, loss/critic2=8.123, n/ep=0, n/st=1, rew=8791.12]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "Epoch #7: test_reward: 8795.913056 ± 59.884823, best_reward: 8795.913056 ± 59.884823 in #7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #8: 501it [00:18, 27.18it/s, env_step=4000, gradient_step=4000, len=1000, loss/actor=-430.551, loss/critic1=8.050, loss/critic2=8.049, n/ep=1, n/st=1, rew=8749.59]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "Epoch #8: test_reward: 8731.267395 ± 47.452960, best_reward: 8795.913056 ± 59.884823 in #7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #9: 501it [00:18, 27.44it/s, env_step=4500, gradient_step=4500, len=1000, loss/actor=-430.542, loss/critic1=7.991, loss/critic2=7.992, n/ep=0, n/st=1, rew=8749.59]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "Epoch #9: test_reward: 8754.329930 ± 37.504617, best_reward: 8795.913056 ± 59.884823 in #7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #10: 501it [00:18, 27.38it/s, env_step=5000, gradient_step=5000, len=1000, loss/actor=-429.997, loss/critic1=7.929, loss/critic2=7.928, n/ep=1, n/st=1, rew=8850.81]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "Epoch #10: test_reward: 8772.517844 ± 48.090063, best_reward: 8795.913056 ± 59.884823 in #7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #11: 501it [00:18, 27.47it/s, env_step=5500, gradient_step=5500, len=1000, loss/actor=-429.824, loss/critic1=7.874, loss/critic2=7.874, n/ep=0, n/st=1, rew=8850.81]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "Epoch #11: test_reward: 8758.346790 ± 53.326115, best_reward: 8795.913056 ± 59.884823 in #7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #12: 501it [00:18, 27.35it/s, env_step=6000, gradient_step=6000, len=1000, loss/actor=-429.689, loss/critic1=7.817, loss/critic2=7.817, n/ep=1, n/st=1, rew=8781.20]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "Epoch #12: test_reward: 8790.454399 ± 19.774976, best_reward: 8795.913056 ± 59.884823 in #7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #13: 501it [00:18, 27.17it/s, env_step=6500, gradient_step=6500, len=1000, loss/actor=-430.104, loss/critic1=7.763, loss/critic2=7.763, n/ep=0, n/st=1, rew=8781.20]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "Epoch #13: test_reward: 8749.315614 ± 44.556904, best_reward: 8795.913056 ± 59.884823 in #7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #14: 501it [00:18, 27.20it/s, env_step=7000, gradient_step=7000, len=1000, loss/actor=-429.501, loss/critic1=7.703, loss/critic2=7.703, n/ep=1, n/st=1, rew=8780.39]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "Epoch #14: test_reward: 8784.209266 ± 50.146064, best_reward: 8795.913056 ± 59.884823 in #7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #15: 501it [00:18, 27.45it/s, env_step=7500, gradient_step=7500, len=1000, loss/actor=-430.438, loss/critic1=7.657, loss/critic2=7.657, n/ep=0, n/st=1, rew=8780.39]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "Epoch #15: test_reward: 8768.168276 ± 54.867876, best_reward: 8795.913056 ± 59.884823 in #7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #16: 501it [00:18, 26.73it/s, env_step=8000, gradient_step=8000, len=1000, loss/actor=-430.446, loss/critic1=7.624, loss/critic2=7.623, n/ep=1, n/st=1, rew=8739.10]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "Epoch #16: test_reward: 8739.327034 ± 35.339409, best_reward: 8795.913056 ± 59.884823 in #7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #17: 501it [00:18, 27.06it/s, env_step=8500, gradient_step=8500, len=1000, loss/actor=-431.078, loss/critic1=7.583, loss/critic2=7.583, n/ep=0, n/st=1, rew=8739.10]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "Epoch #17: test_reward: 8779.264351 ± 53.568757, best_reward: 8795.913056 ± 59.884823 in #7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #18: 501it [00:18, 26.58it/s, env_step=9000, gradient_step=9000, len=1000, loss/actor=-431.708, loss/critic1=7.530, loss/critic2=7.529, n/ep=1, n/st=1, rew=8795.94]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "Epoch #18: test_reward: 8778.550308 ± 52.349528, best_reward: 8795.913056 ± 59.884823 in #7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #19: 501it [00:18, 26.55it/s, env_step=9500, gradient_step=9500, len=1000, loss/actor=-432.422, loss/critic1=7.509, loss/critic2=7.508, n/ep=0, n/st=1, rew=8795.94]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "Epoch #19: test_reward: 8767.204033 ± 33.981249, best_reward: 8795.913056 ± 59.884823 in #7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #20: 501it [00:18, 27.25it/s, env_step=10000, gradient_step=10000, len=1000, loss/actor=-434.071, loss/critic1=7.451, loss/critic2=7.451, n/ep=1, n/st=1, rew=8756.50]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "Epoch #20: test_reward: 8742.564101 ± 33.016307, best_reward: 8795.913056 ± 59.884823 in #7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #21: 501it [00:18, 27.20it/s, env_step=10500, gradient_step=10500, len=1000, loss/actor=-434.345, loss/critic1=7.415, loss/critic2=7.414, n/ep=0, n/st=1, rew=8756.50]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "Epoch #21: test_reward: 8765.544625 ± 25.098822, best_reward: 8795.913056 ± 59.884823 in #7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #22: 501it [00:18, 27.06it/s, env_step=11000, gradient_step=11000, len=1000, loss/actor=-434.951, loss/critic1=7.384, loss/critic2=7.384, n/ep=1, n/st=1, rew=8833.98]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "Epoch #22: test_reward: 8782.290451 ± 43.840742, best_reward: 8795.913056 ± 59.884823 in #7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #23: 501it [00:18, 27.20it/s, env_step=11500, gradient_step=11500, len=1000, loss/actor=-435.987, loss/critic1=7.360, loss/critic2=7.358, n/ep=0, n/st=1, rew=8833.98]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "Epoch #23: test_reward: 8764.893263 ± 48.899207, best_reward: 8795.913056 ± 59.884823 in #7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #24: 501it [00:18, 27.21it/s, env_step=12000, gradient_step=12000, len=1000, loss/actor=-437.298, loss/critic1=7.294, loss/critic2=7.293, n/ep=1, n/st=1, rew=8775.64]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "Epoch #24: test_reward: 8791.018145 ± 6.776477, best_reward: 8795.913056 ± 59.884823 in #7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #25: 501it [00:18, 27.11it/s, env_step=12500, gradient_step=12500, len=1000, loss/actor=-438.237, loss/critic1=7.250, loss/critic2=7.249, n/ep=0, n/st=1, rew=8775.64]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "Epoch #25: test_reward: 8756.727597 ± 49.753440, best_reward: 8795.913056 ± 59.884823 in #7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #26:  85%|########5 | 426/500 [00:15<00:02, 26.71it/s, env_step=12925, gradient_step=12925, len=1000, loss/actor=-439.577, loss/critic1=7.212, loss/critic2=7.211, n/ep=0, n/st=1, rew=8775.64]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 16\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtianshou\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OffpolicyTrainer\n\u001b[1;32m      3\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mOffpolicyTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmix_policy_r\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_collector\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_collector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_collector\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_collector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstep_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstep_per_collect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepisode_per_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mupdate_per_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_in_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m---> 16\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/user/R901105/dev/my_fork/tianshou/tianshou/trainer/base.py:444\u001b[0m, in \u001b[0;36mBaseTrainer.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_run \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 444\u001b[0m     \u001b[43mdeque\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxlen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# feed the entire iterator into a zero-length deque\u001b[39;00m\n\u001b[1;32m    445\u001b[0m     info \u001b[38;5;241m=\u001b[39m gather_info(\n\u001b[1;32m    446\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_time, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_collector, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_collector,\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_reward, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_reward_std\n\u001b[1;32m    448\u001b[0m     )\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m/data/user/R901105/dev/my_fork/tianshou/tianshou/trainer/base.py:302\u001b[0m, in \u001b[0;36mBaseTrainer.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    299\u001b[0m         result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn/st\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_step)\n\u001b[1;32m    300\u001b[0m         t\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m--> 302\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_update_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m     t\u001b[38;5;241m.\u001b[39mset_postfix(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdata)\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mn \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_fn_flag:\n",
      "File \u001b[0;32m/data/user/R901105/dev/my_fork/tianshou/tianshou/trainer/base.py:511\u001b[0m, in \u001b[0;36mOffpolicyTrainer.policy_update_fn\u001b[0;34m(self, data, result)\u001b[0m\n\u001b[1;32m    509\u001b[0m num_updates \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_per_step \u001b[38;5;241m*\u001b[39m n_collected_steps)\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_updates):\n\u001b[0;32m--> 511\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample_and_update\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_collector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/user/R901105/dev/my_fork/tianshou/tianshou/trainer/base.py:459\u001b[0m, in \u001b[0;36mBaseTrainer._sample_and_update\u001b[0;34m(self, buffer, data)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;66;03m# Note: since sample_size=batch_size, this will perform\u001b[39;00m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;66;03m# exactly one gradient step. This is why we don't need to calculate the\u001b[39;00m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;66;03m# number of gradient steps, like in the on-policy case.\u001b[39;00m\n\u001b[0;32m--> 459\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    460\u001b[0m data\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgradient_step\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_step)})\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_update_data(data, losses)\n",
      "File \u001b[0;32m/data/user/R901105/dev/my_fork/tianshou/tianshou/policy/base.py:346\u001b[0m, in \u001b[0;36mBasePolicy.update\u001b[0;34m(self, sample_size, buffer, **kwargs)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    345\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_fn(batch, buffer, indices)\n\u001b[0;32m--> 346\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_process_fn(batch, buffer, indices)\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[29], line 76\u001b[0m, in \u001b[0;36mMixedPolicy.learn\u001b[0;34m(self, batch, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;66;03m# cede_ctrl = batch.policy.cede_ctrl.cpu().squeeze()\u001b[39;00m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# ensemble_batch = batch[~cede_ctrl]\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;66;03m# offline_batch, _ = offline_data.sample(20*len(batch))\u001b[39;00m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;66;03m# _batch = Batch.cat([online_batch, offline_batch])\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m     info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mensemble\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m info\n",
      "Cell \u001b[0;32mIn[12], line 14\u001b[0m, in \u001b[0;36mEnsemblePolicy.learn\u001b[0;34m(self, batch, **kwargs)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     13\u001b[0m     policy \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicies)\n\u001b[0;32m---> 14\u001b[0m     info \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# for policy in self.policies:\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m#     info = policy.learn(batch)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m info\n",
      "File \u001b[0;32m/data/user/R901105/dev/my_fork/tianshou/tianshou/policy/modelfree/dsac.py:221\u001b[0m, in \u001b[0;36mDSACPolicy.learn\u001b[0;34m(self, batch, **kwargs)\u001b[0m\n\u001b[1;32m    217\u001b[0m taus_hat_j, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_taus(\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_taus, batch\u001b[38;5;241m.\u001b[39mobs\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    219\u001b[0m )\n\u001b[1;32m    220\u001b[0m critic1_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_critic_loss(batch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic1, taus_hat_j)\n\u001b[0;32m--> 221\u001b[0m critic2_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_critic_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcritic2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtaus_hat_j\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic1_optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    223\u001b[0m critic1_loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/data/user/R901105/dev/my_fork/tianshou/tianshou/policy/modelfree/dsac.py:176\u001b[0m, in \u001b[0;36mDSACPolicy._critic_loss\u001b[0;34m(self, batch, critic, taus_hat_j)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_critic_loss\u001b[39m(\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28mself\u001b[39m, batch: RolloutBatchProtocol, critic: torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule, taus_hat_j: torch\u001b[38;5;241m.\u001b[39mTensor\n\u001b[1;32m    175\u001b[0m     ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 176\u001b[0m     current_z \u001b[38;5;241m=\u001b[39m \u001b[43mcritic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtaus_hat_j\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    178\u001b[0m         target_z, presum_taus_i \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_target_z(batch)\n",
      "File \u001b[0;32m/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data/user/R901105/dev/my_fork/tianshou/tianshou/utils/net/continuous.py:558\u001b[0m, in \u001b[0;36mQuantileMlp.forward\u001b[0;34m(self, state, action, tau)\u001b[0m\n\u001b[1;32m    556\u001b[0m h \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmul(x, h\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m))  \u001b[38;5;66;03m# (N, T, C)\u001b[39;00m\n\u001b[1;32m    557\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmerge_fc(h)  \u001b[38;5;66;03m# (N, T, C)\u001b[39;00m\n\u001b[0;32m--> 558\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlast_fc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (N, T)\u001b[39;00m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tianshou.trainer import OffpolicyTrainer\n",
    "\n",
    "result = OffpolicyTrainer(\n",
    "    policy=mix_policy_r,\n",
    "    train_collector=train_collector,\n",
    "    test_collector=test_collector,\n",
    "    test_fn=test_fn,\n",
    "    max_epoch=200,\n",
    "    step_per_epoch=500,\n",
    "    step_per_collect=1,\n",
    "    episode_per_test=5,\n",
    "    batch_size=1024,\n",
    "    logger=logger,\n",
    "    update_per_step=1,\n",
    "    test_in_train=False,\n",
    ").run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[545.0673, 562.5686, 568.5168, 571.4066, 575.6000, 579.4008, 582.0373,\n",
       "           585.4357, 588.5088, 591.5640, 592.9493, 594.2272, 595.8409, 598.3125,\n",
       "           601.0197, 605.1782]],\n",
       " \n",
       "         [[546.1712, 553.7066, 567.0347, 579.1821, 586.2369, 590.5028, 594.5225,\n",
       "           596.2476, 597.4871, 598.3288, 599.1767, 599.7960, 600.3356, 600.6666,\n",
       "           600.8529, 601.9847]],\n",
       " \n",
       "         [[537.5802, 559.9738, 571.1710, 578.7217, 584.5421, 588.6882, 591.4799,\n",
       "           593.8772, 594.7488, 596.7670, 598.7935, 600.2566, 602.1553, 605.3320,\n",
       "           607.4578, 611.3715]]], device='cuda:0'),\n",
       " tensor([[725.7118, 730.0748, 733.0227, 734.5841, 735.5822, 736.1691, 736.8743,\n",
       "          737.5449, 737.7747, 738.0643, 738.3610, 738.6759, 738.6823, 739.0128,\n",
       "          739.3438, 741.5106]], device='cuda:0'))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mix_policy_r.q_values, mix_policy_r.expert_q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch, _ = test_buffer.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_policy(batch).policy.cede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n/ep': 40,\n",
       " 'n/st': 40000,\n",
       " 'rews': array([5955.42561977, 5960.14295851, 6345.67900197, 6452.10139581,\n",
       "        6212.52859869, 6550.26161751, 6066.07010389, 6433.33568906,\n",
       "        6350.11304232, 6544.81412222, 5570.7515739 , 6228.13801271,\n",
       "        6050.16460722, 4135.43621449, 6278.55061001, 6330.75545796,\n",
       "        6357.43226971, 6547.04719519, 6617.90720573, 6404.36953168,\n",
       "        6328.90534876, 6366.14252353, 5955.41433545, 2509.63836869,\n",
       "        6481.64620236, 6681.59705898, 6206.2879277 , 6255.91152185,\n",
       "        6557.70295486, 6798.25913636, 6215.51608019, 4775.20809866,\n",
       "        6309.71408314, 6464.83685722, 5958.31083627, 6403.75437122,\n",
       "        6112.81654475, 6278.83882244, 5816.52311178, 6163.08860933]),\n",
       " 'lens': array([1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000,\n",
       "        1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000,\n",
       "        1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000,\n",
       "        1000, 1000, 1000, 1000, 1000, 1000, 1000]),\n",
       " 'idxs': array([0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1,\n",
       "        2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4]),\n",
       " 'rew': 6100.77844054742,\n",
       " 'len': 1000.0,\n",
       " 'rew_std': 743.4585236728153,\n",
       " 'len_std': 0.0}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "offline_policy1.eval()\n",
    "test_collector1 = Collector(offline_policy1, test_envs)\n",
    "test_collector1.collect(n_episode=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n/ep': 40,\n",
       " 'n/st': 40000,\n",
       " 'rews': array([7399.17203028, 7914.14273599, 7544.20808021, 7636.57072513,\n",
       "         904.89063934, 7735.46514638, 7901.57782222, 7947.22028961,\n",
       "        7745.99208287, 7258.08251182, 7973.68930226, 7762.74729772,\n",
       "        7841.8240032 , 1566.82583688, 1484.69256004, 1712.70866913,\n",
       "        7959.81524642, 7551.93866188, 2019.27911924, 7425.17029724,\n",
       "        7608.51486709,  727.6945203 , 7762.93666155, 6418.43275534,\n",
       "        1825.18678502, 1936.20696587, 7665.64240641, 7609.50478503,\n",
       "         599.280254  , 7696.94675348, 1969.34297812, 7615.03992924,\n",
       "        5491.1139712 ,  379.65830981, 6126.70561984, 6287.92708114,\n",
       "        2473.01080446, 7729.44149076, 4079.78395818, 7607.87053522]),\n",
       " 'lens': array([1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000,\n",
       "        1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000,\n",
       "        1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000,\n",
       "        1000, 1000, 1000, 1000, 1000, 1000, 1000]),\n",
       " 'idxs': array([0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1,\n",
       "        2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4]),\n",
       " 'rew': 5572.406362247668,\n",
       " 'len': 1000.0,\n",
       " 'rew_std': 2805.58692363643,\n",
       " 'len_std': 0.0}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_collector2 = Collector(offline_policy2, test_envs)\n",
    "test_collector2.collect(n_episode=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n/ep': 40,\n",
       " 'n/st': 40000,\n",
       " 'rews': array([8157.6341626 , 8119.1089945 , 8140.25654648, 8112.01465062,\n",
       "        8087.14475744, 8249.26421919, 8227.97061908, 8163.37250221,\n",
       "        8224.49698425, 8111.75105617, 8136.04424887, 8254.110531  ,\n",
       "        8310.03384418, 8144.18041288, 8160.54208639, 8031.82265691,\n",
       "        8143.09030891, 8219.48163546, 8154.22866723, 8202.2997809 ,\n",
       "        8218.79211063, 8229.49005346, 8203.24031099, 8277.30651372,\n",
       "        8180.32811725, 8199.3881451 , 8132.55991737, 7930.75162308,\n",
       "        8003.96829416, 8288.30757471, 8137.40215464, 8096.91538232,\n",
       "        8142.19346779, 8130.72992741, 8009.46286966, 8129.60143044,\n",
       "        8056.0423756 , 8179.87556251, 8278.53934503, 8245.24927283]),\n",
       " 'lens': array([1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000,\n",
       "        1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000,\n",
       "        1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000,\n",
       "        1000, 1000, 1000, 1000, 1000, 1000, 1000]),\n",
       " 'idxs': array([0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1,\n",
       "        2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4]),\n",
       " 'rew': 8160.474827849405,\n",
       " 'len': 1000.0,\n",
       " 'rew_std': 81.3620913441827,\n",
       " 'len_std': 0.0}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_collector3 = Collector(offline_policy3, test_envs)\n",
    "test_collector3.collect(n_episode=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
