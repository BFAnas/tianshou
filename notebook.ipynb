{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load offline trained agent\n",
    "# load expert agent\n",
    "# write ensemble class for offline trained agents\n",
    "# use distributional RL to detect risky states\n",
    "# use ensembles to detect novel states\n",
    "# if novelty is above a treshold give control to expert \n",
    "# if risk is above a treshold give control to expert\n",
    "# can conformal prediction give us guaranties about the performance in this setu?p\n",
    "# empirecally verify if we are able to get the desired performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from types import SimpleNamespace\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from examples.offline.utils import load_buffer_d4rl\n",
    "from tianshou.policy import DSACPolicy, BasePolicy\n",
    "from tianshou.data.buffer.vecbuf import VectorReplayBuffer\n",
    "from tianshou.env import SubprocVectorEnv\n",
    "from tianshou.data import Collector, Batch, to_torch\n",
    "from tianshou.data.types import RolloutBatchProtocol\n",
    "from tianshou.utils.net.common import Net\n",
    "from tianshou.utils.net.continuous import ActorProb, QuantileMlp\n",
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "seed = 0\n",
    "np.random.seed(seed);\n",
    "torch.manual_seed(seed);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_value(value):\n",
    "    # Convert simple types (int, float, bool, None)\n",
    "    if value.isdigit():\n",
    "        return int(value)\n",
    "    elif re.match(r'^\\d+\\.\\d+$', value):\n",
    "        return float(value)\n",
    "    elif value == \"True\":\n",
    "        return True\n",
    "    elif value == \"False\":\n",
    "        return False\n",
    "    elif value == \"None\":\n",
    "        return None\n",
    "    elif value.startswith(\"[\") and value.endswith(\"]\"):\n",
    "        # Convert the list items\n",
    "        items = re.split(r',(?=[^\\]]*(?:\\[|$))', value[1:-1])\n",
    "        return [parse_value(item.strip()) for item in items]\n",
    "    elif value.startswith(\"(\") and value.endswith(\")\"):\n",
    "        # Convert the tuple items\n",
    "        items = re.split(r',(?=[^\\)]*(?:\\(|$))', value[1:-1])\n",
    "        # Special case for single-item tuple\n",
    "        if len(items) == 2 and items[0].strip() != '':\n",
    "            return (parse_value(items[0].strip()),)\n",
    "        return tuple(parse_value(item.strip()) for item in items)\n",
    "    elif value.startswith(\"'\") and value.endswith(\"'\"):\n",
    "        return value[1:-1]\n",
    "    # Else, return the value as-is\n",
    "    return value\n",
    "\n",
    "def get_args(event_file):\n",
    "    ea = EventAccumulator(event_file)\n",
    "    ea.Reload()  # Load the file\n",
    "    # Get the text data\n",
    "    texts = ea.Tags()[\"tensors\"]\n",
    "    # Extract the actual text content\n",
    "    text_data = {}\n",
    "    for tag in texts:\n",
    "        events = ea.Tensors(tag)\n",
    "        for event in events:\n",
    "            # You can extract the wall_time and step if needed\n",
    "            # wall_time, step, value = event.wall_time, event.step, event.text\n",
    "            text_data[tag] = event.tensor_proto.string_val\n",
    "    data = text_data['args/text_summary'][0]\n",
    "    # Convert bytes to string\n",
    "    data_str = data.decode('utf-8')\n",
    "    # Remove the \"Namespace(\" prefix and the trailing \")\"\n",
    "    data_str = data_str[len(\"Namespace(\"):-1]\n",
    "    # Split into key-value pairs\n",
    "    key_values = re.split(r',(?=\\s\\w+=)', data_str)\n",
    "    # Parse each key-value pair\n",
    "    args_dict = {}\n",
    "    for kv in key_values:\n",
    "        key, value = kv.split('=', 1)\n",
    "        key = key.strip()\n",
    "        args_dict[key] = parse_value(value)\n",
    "    args = SimpleNamespace(**args_dict)\n",
    "    try:\n",
    "        env = gym.make(args.task)\n",
    "        target_entropy = -np.prod(env.action_space.shape)\n",
    "        log_alpha = torch.zeros(1, requires_grad=True, device=device)\n",
    "        alpha_optim = torch.optim.Adam([log_alpha], lr=args.alpha_lr)\n",
    "        args.alpha = (target_entropy, log_alpha, alpha_optim)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dsac_args():\n",
    "    args = argparse.Namespace(\n",
    "        task=\"Hopper-v2\",\n",
    "        risk_type=\"wang\",\n",
    "        buffer_size=1000000,\n",
    "        # hidden_sizes=[256, 256, 256],\n",
    "        hidden_sizes=[256, 256],\n",
    "        actor_lr=3e-4,\n",
    "        critic_lr=3e-4,\n",
    "        gamma=0.99,\n",
    "        tau=0.005,\n",
    "        alpha=0.55,\n",
    "        start_timesteps=1,\n",
    "        epoch=200,\n",
    "        step_per_epoch=5000,\n",
    "        step_per_collect=1,\n",
    "        update_per_step=1,\n",
    "        batch_size=256,\n",
    "        training_num=1,\n",
    "        test_num=10,\n",
    "        distortion_param=0.75,\n",
    "    )\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_policy(args, path):\n",
    "    env = gym.make(args.task)\n",
    "    args.state_shape = env.observation_space.shape or env.observation_space.n\n",
    "    args.action_shape = env.action_space.shape or env.action_space.n\n",
    "    # model\n",
    "    net_a = Net(args.state_shape, hidden_sizes=args.hidden_sizes, device=device)\n",
    "    actor = ActorProb(\n",
    "        net_a,\n",
    "        args.action_shape,\n",
    "        device=device,\n",
    "        unbounded=True,\n",
    "        conditioned_sigma=True,\n",
    "    ).to(device)\n",
    "    actor_optim = torch.optim.Adam(actor.parameters(), lr=args.actor_lr)\n",
    "    critic1 = QuantileMlp(hidden_sizes=args.hidden_sizes, input_size=args.state_shape[0] + args.action_shape[0], device=device).to(device)\n",
    "    critic1_optim = torch.optim.Adam(critic1.parameters(), lr=args.critic_lr)\n",
    "    critic2 = QuantileMlp(hidden_sizes=args.hidden_sizes, input_size=args.state_shape[0] + args.action_shape[0], device=device).to(device)\n",
    "    critic2_optim = torch.optim.Adam(critic2.parameters(), lr=args.critic_lr)\n",
    "    policy = DSACPolicy(\n",
    "        actor,\n",
    "        actor_optim,\n",
    "        critic1,\n",
    "        critic1_optim,\n",
    "        critic2,\n",
    "        critic2_optim,\n",
    "        risk_type='wang',\n",
    "        tau=args.tau,\n",
    "        gamma=args.gamma,\n",
    "        alpha=0.4,\n",
    "        action_space=env.action_space,\n",
    "        device=device,\n",
    "        distortion_param=0.75,\n",
    "    )\n",
    "    dirname = os.path.dirname(path)\n",
    "    if os.path.isfile(os.path.join(dirname, \"actor.pth\")):\n",
    "        policy.actor.load_state_dict(torch.load(os.path.join(dirname, \"actor.pth\"), map_location=device))\n",
    "        print(\"Loaded actor from: \", os.path.join(dirname, \"actor.pth\"))\n",
    "    if os.path.isfile(os.path.join(dirname, \"critic1.pth\")):\n",
    "        policy.critic1.load_state_dict(torch.load(os.path.join(dirname, \"critic1.pth\"), map_location=device))\n",
    "        policy.critic1_old.load_state_dict(torch.load(os.path.join(dirname, \"critic1.pth\"), map_location=device))\n",
    "        print(\"Loaded critic1 from: \", os.path.join(dirname, \"critic1.pth\"))\n",
    "    if os.path.isfile(os.path.join(dirname, \"critic2.pth\")):\n",
    "        policy.critic2.load_state_dict(torch.load(os.path.join(dirname, \"critic2.pth\"), map_location=device))\n",
    "        policy.critic2_old.load_state_dict(torch.load(os.path.join(dirname, \"critic2.pth\"), map_location=device))\n",
    "        print(\"Loaded critic2 from: \", os.path.join(dirname, \"critic2.pth\"))\n",
    "    else:\n",
    "        policy.load_state_dict(torch.load(path, map_location=device))\n",
    "        print(\"Loaded agent from: \", path)\n",
    "    return policy\n",
    "\n",
    "def load_behavioral_crtitic(args, path):\n",
    "    behavioral_critic = QuantileMlp(\n",
    "        input_size=args.state_shape[0] + args.action_shape[0],\n",
    "        hidden_sizes=args.hidden_sizes,\n",
    "        device=device,\n",
    "    ).to(device)\n",
    "    behavioral_critic.load_state_dict(torch.load(path, map_location=device))\n",
    "    return behavioral_critic\n",
    "\n",
    "def get_model(log_path, type=None):\n",
    "    files = os.listdir(log_path)\n",
    "    event_file = [f for f in files if f.startswith('event')][0]\n",
    "    full_path = os.path.join(log_path, event_file)\n",
    "    if type == \"behavioral\":\n",
    "        args = get_args(full_path)\n",
    "        resume_path = os.path.join(log_path, 'model.pth')\n",
    "        policy = load_behavioral_crtitic(args, resume_path)\n",
    "    elif type == \"codac\":\n",
    "        args = get_args(full_path)\n",
    "        resume_path = os.path.join(log_path, 'policy.pth')\n",
    "        policy = load_policy(args, resume_path)\n",
    "    else:\n",
    "        args = get_dsac_args()\n",
    "        resume_path = os.path.join(log_path, 'policy.pth')\n",
    "        policy = load_policy(args, resume_path)\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = \"/data/user/R901105/dev/log/Hopper-v4/qr/231102-133240\"\n",
    "behavioral_critic = get_model(log_path, \"behavioral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded actor from:  /data/user/R901105/dev/log/Hopper-v2/codac_bc/neutral/0/231102-150037/actor.pth\n",
      "Loaded critic1 from:  /data/user/R901105/dev/log/Hopper-v2/codac_bc/neutral/0/231102-150037/critic1.pth\n",
      "Loaded critic2 from:  /data/user/R901105/dev/log/Hopper-v2/codac_bc/neutral/0/231102-150037/critic2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/gymnasium/envs/registration.py:513: DeprecationWarning: \u001b[33mWARN: The environment Hopper-v2 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
      "  logger.deprecation(\n",
      "/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/gymnasium/envs/mujoco/mujoco_env.py:211: DeprecationWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n",
      "  logger.deprecation(\n"
     ]
    }
   ],
   "source": [
    "log_path1 = \"/data/user/R901105/dev/log/Hopper-v2/codac_bc/neutral/0/231102-150037\"\n",
    "offline_policy1 = get_model(log_path1, \"codac\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded actor from:  /data/user/R901105/dev/log/Hopper-v2/codac_bc/neutral/0/231220-190802/actor.pth\n",
      "Loaded critic1 from:  /data/user/R901105/dev/log/Hopper-v2/codac_bc/neutral/0/231220-190802/critic1.pth\n",
      "Loaded critic2 from:  /data/user/R901105/dev/log/Hopper-v2/codac_bc/neutral/0/231220-190802/critic2.pth\n"
     ]
    }
   ],
   "source": [
    "log_path2 = \"/data/user/R901105/dev/log/Hopper-v2/codac_bc/neutral/0/231220-190802\"\n",
    "offline_policy2 = get_model(log_path2, \"codac\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded actor from:  /data/user/R901105/dev/log/Hopper-v2/codac_bc/neutral/0/231113-170715/actor.pth\n",
      "Loaded critic1 from:  /data/user/R901105/dev/log/Hopper-v2/codac_bc/neutral/0/231113-170715/critic1.pth\n",
      "Loaded critic2 from:  /data/user/R901105/dev/log/Hopper-v2/codac_bc/neutral/0/231113-170715/critic2.pth\n"
     ]
    }
   ],
   "source": [
    "log_path3 = \"/data/user/R901105/dev/log/Hopper-v2/codac_bc/neutral/0/231113-170715\"\n",
    "offline_policy3 = get_model(log_path3, \"codac\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded agent from:  /data/user/R901105/dev/log/Hopper-v4/dsac/wang/0/230824-151635/policy.pth\n"
     ]
    }
   ],
   "source": [
    "log_path = \"/data/user/R901105/dev/log/Hopper-v4/dsac/wang/0/230824-151635\"\n",
    "expert_policy = get_model(log_path, \"codac\") \n",
    "expert_policy.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "class EnsemblePolicy(BasePolicy):\n",
    "    def __init__(self, policies, action_space):\n",
    "        super().__init__(action_space=action_space)\n",
    "        self.policies = policies\n",
    "\n",
    "    def forward(self, batch, state=None, **kwargs):\n",
    "        return self.policies[-1](batch)\n",
    "    \n",
    "    def learn(self, batch, **kwargs):\n",
    "        policy = random.choice(self.policies)\n",
    "        info = policy.learn(batch)\n",
    "        # for policy in self.policies:\n",
    "        #     info = policy.learn(batch)\n",
    "        return info\n",
    "    \n",
    "    def bc(self, batch):\n",
    "        policy = random.choice(self.policies)\n",
    "        pred_act = policy.actor(batch.obs)\n",
    "        expert_act = batch.act\n",
    "        loss = (pred_act - expert_act).pow(2).mean()\n",
    "        policy.actor_optim.zero_grad()\n",
    "        loss.backward()\n",
    "        policy.actor_optim.step()\n",
    "        return loss\n",
    "\n",
    "    def get_qvalues(self, obs, act):\n",
    "        q_values = torch.stack([p.critic1(obs, act).detach() for p in self.policies])\n",
    "        return q_values\n",
    "    \n",
    "    def train(self, mode: bool = True) -> \"EnsemblePolicy\":\n",
    "        for policy in self.policies:\n",
    "            policy.eval()\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeterministicPolicy(BasePolicy):\n",
    "    def __init__(self, policy, action_space):\n",
    "        super().__init__(action_space=action_space)\n",
    "        self.policy = policy\n",
    "\n",
    "    def train(self, mode: bool = True) -> \"DeterministicPolicy\":\n",
    "        self.policy.eval()\n",
    "        return self\n",
    "    \n",
    "    def forward(self, batch, state=None, **kwargs):\n",
    "        return self.policy(batch)\n",
    "    \n",
    "    def learn(self, batch, **kwargs):\n",
    "        info = self.policy.learn(batch)\n",
    "        return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"Hopper-v2\"\n",
    "env_num = 20\n",
    "env = gym.make(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "policies = [offline_policy1, offline_policy3]\n",
    "# policies = [offline_policy2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = EnsemblePolicy(policies, env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# give control to the expert policy based on epistemic and aleatoric uncertainties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixedPolicy(BasePolicy):\n",
    "    def __init__(self, ensemble: EnsemblePolicy, expert_policy: BasePolicy, old_critic: QuantileMlp, action_space, config=\"nr\"):\n",
    "        super().__init__(action_space=action_space)\n",
    "        self.ensemble = ensemble\n",
    "        self.expert_policy = expert_policy\n",
    "        self.old_critic = old_critic\n",
    "        self.config = config\n",
    "\n",
    "    def update_results(self, batch: RolloutBatchProtocol):\n",
    "        self.ensemble_result = self.ensemble(batch)\n",
    "        self.expert_result = self.expert_policy(batch)\n",
    "        actions = self.ensemble_result.act\n",
    "        expert_actions = self.expert_result.act\n",
    "        self.q_values = self.ensemble.get_qvalues(batch.obs, actions) # (n_policies, bsz, n_taus)\n",
    "        self.expert_q_values = self.expert_policy.critic1(batch.obs, expert_actions)\n",
    "\n",
    "    def update_novelty_threshold(self, buffer, step=0, decay=0.9, start=0.89):\n",
    "        batch, _ = buffer.sample(25000)\n",
    "        batch = to_torch(batch, dtype=torch.float32, device=device)\n",
    "        q_values = self.ensemble.get_qvalues(batch.obs, batch.act)\n",
    "        data_epistemic_uncert = torch.std(q_values, 0).mean(-1).detach().cpu().numpy()\n",
    "        pecentile = (1 -start) * (decay**step)\n",
    "        self.novelty_threshold = np.quantile(data_epistemic_uncert, pecentile)\n",
    "\n",
    "    def get_uncertainties(self):\n",
    "        epistemic_uncert = torch.std(self.q_values, 0).mean(-1) # bsz\n",
    "        aleatoric_uncert = torch.std(self.q_values, -1).mean(0) # bsz\n",
    "        return epistemic_uncert, aleatoric_uncert\n",
    "\n",
    "    def cede_control(self):\n",
    "        epistemic_uncert, aleatoric_uncert = self.get_uncertainties()\n",
    "        novelty = epistemic_uncert > self.novelty_threshold\n",
    "        risk = (self.q_values.mean((0, 2)) - aleatoric_uncert) < self.expert_q_values.mean(-1)\n",
    "        return novelty, risk\n",
    "\n",
    "    # def cede_control(self):\n",
    "    #     risk = self.q_values.mean((0, 2)) < 0.9*self.expert_q_values.mean(-1)\n",
    "    #     return None , risk\n",
    "\n",
    "    def take_control(self):\n",
    "        ensemble_actions = self.ensemble_result.act\n",
    "        expert_actions = self.expert_result.act\n",
    "        expert_std = self.expert_result.dist.base_dist.scale\n",
    "        return torch.any(abs(ensemble_actions-expert_actions) < expert_std/2, dim=1)\n",
    "\n",
    "    def forward(self, batch: RolloutBatchProtocol, state=None, **kwargs):\n",
    "        batch = to_torch(batch, dtype=torch.float32, device=device)\n",
    "        self.update_results(batch)\n",
    "        novelty, risk = self.cede_control()\n",
    "        if self.config == \"n\":\n",
    "            cede_ctrl = novelty\n",
    "        elif self.config == \"r\":\n",
    "            cede_ctrl = risk\n",
    "        elif self.config == \"nr\":\n",
    "            cede_ctrl = torch.logical_or(novelty, risk)\n",
    "        else:\n",
    "            cede_ctrl = torch.zeros_like(novelty)\n",
    "        cede_ctrl = cede_ctrl.unsqueeze(-1)\n",
    "        actions = torch.where(cede_ctrl, self.expert_result.act, self.ensemble_result.act)\n",
    "        return Batch(**{'act': actions, 'policy': Batch({'cede_ctrl': cede_ctrl})})\n",
    "\n",
    "    def train(self, mode: bool = True) -> \"MixedPolicy\":\n",
    "        self.ensemble.train(mode)\n",
    "        return self\n",
    "\n",
    "    def learn(self, batch, **kwargs):\n",
    "        # cede_ctrl = batch.policy.cede_ctrl.cpu().squeeze()\n",
    "        # ensemble_batch = batch[~cede_ctrl]\n",
    "        # if len(ensemble_batch) > 0:\n",
    "        #     info = self.ensemble.learn(ensemble_batch)\n",
    "        # else:\n",
    "        #     info = {}\n",
    "        # online_batch = batch\n",
    "        # offline_batch, _ = offline_data.sample(20*len(batch))\n",
    "        # _batch = Batch.cat([online_batch, offline_batch])\n",
    "        info = self.ensemble.learn(batch)\n",
    "        return info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# estimate novelty_threshold from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/gym/envs/mujoco/mujoco_env.py:190: UserWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n",
      "  logger.warn(\n",
      "/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/gym/spaces/box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "load datafile: 100%|██████████| 21/21 [00:01<00:00, 17.72it/s]\n"
     ]
    }
   ],
   "source": [
    "offline_data = load_buffer_d4rl(\"hopper-medium-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tianshou.data.buffer.base import ReplayBuffer\n",
    "\n",
    "# def add_policy_id(buffer: ReplayBuffer) -> ReplayBuffer:\n",
    "#     data_dict = buffer._meta.__dict__\n",
    "#     new_data_dict = data_dict.copy()\n",
    "#     new_data_dict[\"policy\"] = Batch(**{'id': np.full(buffer.rew.shape, -1)})\n",
    "#     new_batch = Batch(**new_data_dict)\n",
    "#     buffer._meta = new_batch\n",
    "#     return buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# offline_data = add_policy_id(offline_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch, _ = offline_data.sample(10)\n",
    "# batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "offline_batch, _ = offline_data.sample(25000)\n",
    "offline_batch = to_torch(offline_batch, dtype=torch.float32, device=device)\n",
    "q_values = ensemble.get_qvalues(offline_batch.obs, offline_batch.act)\n",
    "data_epistemic_uncert = torch.std(q_values, 0).mean(-1).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_bins = int(np.sqrt(data_epistemic_uncert.size))\n",
    "data_range = (np.min(data_epistemic_uncert), np.max(data_epistemic_uncert))\n",
    "quantile_89 = np.quantile(data_epistemic_uncert, 0.89)\n",
    "quantile_99 = np.quantile(data_epistemic_uncert, 0.99)\n",
    "# Plotting the histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(data_epistemic_uncert, bins=num_bins, range=data_range, color='blue', edgecolor='black', alpha=0.7)\n",
    "# Plot a vertical line for the 89% quantile\n",
    "plt.axvline(quantile_89, color='red', linestyle='dashed', linewidth=2)\n",
    "plt.axvline(quantile_99, color='green', linestyle='dashed', linewidth=2)\n",
    "# Annotate the 89% quantile value on the plot\n",
    "plt.text(quantile_89, plt.ylim()[1]*0.9, f'89th Quantile: {quantile_89:.2f}', color='red')\n",
    "plt.text(quantile_99, plt.ylim()[1]*0.75, f'99th Quantile: {quantile_99:.2f}', color='green')\n",
    "plt.title('Histogram')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Establish baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_policy_nr = MixedPolicy(ensemble, expert_policy, behavioral_critic, env.action_space)\n",
    "mix_policy_n = MixedPolicy(ensemble, expert_policy, behavioral_critic, env.action_space, \"n\")\n",
    "mix_policy = MixedPolicy(ensemble, expert_policy, behavioral_critic, env.action_space, \"\")\n",
    "mix_policy_nr.update_novelty_threshold(offline_data)\n",
    "mix_policy_n.update_novelty_threshold(offline_data)\n",
    "mix_policy.update_novelty_threshold(offline_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = SubprocVectorEnv([lambda: gym.make(task) for _ in range(20)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'envs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/data/user/R901105/dev/my_fork/tianshou/notebook.ipynb Cell 28\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224f41532d333536227d/data/user/R901105/dev/my_fork/tianshou/notebook.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m mix_buffer_n \u001b[39m=\u001b[39m VectorReplayBuffer(\u001b[39m20000\u001b[39m, \u001b[39m20\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224f41532d333536227d/data/user/R901105/dev/my_fork/tianshou/notebook.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m mix_buffer_nr \u001b[39m=\u001b[39m VectorReplayBuffer(\u001b[39m20000\u001b[39m, \u001b[39m20\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224f41532d333536227d/data/user/R901105/dev/my_fork/tianshou/notebook.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m ens_collector \u001b[39m=\u001b[39m Collector(ensemble, envs)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224f41532d333536227d/data/user/R901105/dev/my_fork/tianshou/notebook.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m exp_collector \u001b[39m=\u001b[39m Collector(expert_policy, envs)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224f41532d333536227d/data/user/R901105/dev/my_fork/tianshou/notebook.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m mix_collector \u001b[39m=\u001b[39m Collector(mix_policy, envs, mix_buffer)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'envs' is not defined"
     ]
    }
   ],
   "source": [
    "mix_buffer = VectorReplayBuffer(20000, 20)\n",
    "mix_buffer_n = VectorReplayBuffer(20000, 20)\n",
    "mix_buffer_nr = VectorReplayBuffer(20000, 20)\n",
    "\n",
    "ens_collector = Collector(ensemble, envs)\n",
    "exp_collector = Collector(expert_policy, envs)\n",
    "mix_collector = Collector(mix_policy, envs, mix_buffer)\n",
    "mix_collector_n = Collector(mix_policy_n, envs, mix_buffer_n)\n",
    "mix_collector_nr = Collector(mix_policy_nr, envs, mix_buffer_nr)\n",
    "\n",
    "ens_result = ens_collector.collect(n_episode=40)\n",
    "exp_result = exp_collector.collect(n_episode=40)\n",
    "mix_result = mix_collector.collect(n_episode=40)\n",
    "mix_result_n = mix_collector_n.collect(n_episode=40)\n",
    "mix_result_nr = mix_collector_nr.collect(n_episode=40)\n",
    "\n",
    "ens_baseline = ens_result[\"rew\"]\n",
    "\n",
    "ens_result[\"rew\"], exp_result[\"rew\"], mix_result[\"rew\"], mix_result_n[\"rew\"], mix_result_nr[\"rew\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.), tensor(0.2531), tensor(0.4770))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mix_buffer.policy.cede_ctrl.cpu().float().mean(), mix_buffer_n.policy.cede_ctrl.cpu().float().mean(), mix_buffer_nr.policy.cede_ctrl.cpu().float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4770)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch, _ = mix_buffer_nr.sample(0)\n",
    "batch.policy.cede_ctrl.cpu().float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n/ep': 40,\n",
       " 'n/st': 30484,\n",
       " 'rews': array([ 218.67724322, 1858.98361737, 1871.46306909, 2047.87863298,\n",
       "        2142.85623981,  202.35395675, 2143.27394154, 2182.93209263,\n",
       "        2254.38246502, 2330.12242126, 2016.30572596, 2368.57083753,\n",
       "        2447.32575643, 2551.91343293, 2640.43396596, 2766.65066257,\n",
       "        2736.80541416, 2829.74075304,  786.07354966, 3118.27276332,\n",
       "        3120.70864751, 2854.70439358, 3065.09742574, 1786.47047476,\n",
       "        2014.47356902, 1613.57576491, 1996.07402347, 2014.91071243,\n",
       "        1692.28960851, 2620.68995003, 2643.49279231, 2633.0848968 ,\n",
       "        2941.4182619 , 2419.02521637, 3142.57673487, 2929.34489188,\n",
       "        2980.11757391, 3010.35864871, 3026.05335863, 2774.23748434]),\n",
       " 'lens': array([ 121,  591,  618,  670,  684,  113,  711,  726,  743,  750,  643,\n",
       "         781,  792,  858,  867,  867,  883,  917,  260, 1000, 1000, 1000,\n",
       "        1000,  590,  652,  528,  658,  660,  572,  873,  831,  872, 1000,\n",
       "         768, 1000, 1000, 1000, 1000, 1000,  885]),\n",
       " 'idxs': array([ 3,  7, 14, 18, 19,  7,  5, 16,  6,  2,  3,  0,  8, 10,  4, 15, 11,\n",
       "        13,  5,  1,  9, 12, 17, 14,  7,  4,  6, 15,  5, 18, 16,  0, 19, 13,\n",
       "         2,  3,  8, 10, 11, 17]),\n",
       " 'rew': 2319.8430242727045,\n",
       " 'len': 762.1,\n",
       " 'rew_std': 701.2400912625507,\n",
       " 'len_std': 223.7696136654841}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "off_collector = Collector(offline_policy2, envs)\n",
    "off_result = off_collector.collect(n_episode=40)\n",
    "off_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check the performance of the ensemble + expert in comparaison with only the ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policies(ensemble, expert_policy, behavioral_critic, novelty_threshold, n_episode):\n",
    "    mix_policy = MixedPolicy(ensemble, expert_policy, behavioral_critic, env.action_space, novelty_threshold)\n",
    "    mix_collector = Collector(mix_policy, env)\n",
    "    mix_result = mix_collector.collect(n_episode=n_episode)\n",
    "    improvement = (mix_result['rew'] - ens_baseline) / mix_result['rew']\n",
    "    return mix_result['rew'], mix_policy.expert_calls.mean(), improvement    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_discounted_accumulated_rewards(policy: BasePolicy, env_name: str, gamma: float) -> float:\n",
    "    \"\"\"\n",
    "    Run a Tianshou policy in a given environment and calculate the discounted accumulated rewards.\n",
    "\n",
    "    :param policy: Tianshou policy to be evaluated.\n",
    "    :param env_name: Name of the gym environment.\n",
    "    :param gamma: Discount factor for future rewards.\n",
    "    :return: Discounted accumulated reward.\n",
    "    \"\"\"\n",
    "    # Create the environment\n",
    "    env = gym.make(env_name)\n",
    "    state, _ = env.reset()\n",
    "\n",
    "    done = False\n",
    "    discounted_accumulated_reward = 0\n",
    "    accumulated_reward = 0\n",
    "    discount = 1\n",
    "\n",
    "    # Run the policy in the environment\n",
    "    while not done:\n",
    "        # Predict the action given the current state\n",
    "        action = policy(Batch(**{\"obs\": state[np.newaxis, ...], \"info\": 0})).act.squeeze().detach().cpu()\n",
    "\n",
    "        # Take the action in the environment\n",
    "        next_state, reward, truncated, terminated, _ = env.step(action)\n",
    "\n",
    "        # Update the discounted accumulated reward\n",
    "        discounted_accumulated_reward += reward * discount\n",
    "        accumulated_reward += reward\n",
    "        discount *= gamma\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "        done = truncated or terminated\n",
    "\n",
    "    env.close()\n",
    "    return discounted_accumulated_reward, accumulated_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_discounted_accumulated_rewards(expert_policy, \"Hopper-v2\", 0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# perform continual learning and observe the evolution of numbers of calls to the expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this can be tricky as we need to do continual learning for all the policies in the ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.seed to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.seed` for environment variables or `env.get_wrapper_attr('seed')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.seed to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.seed` for environment variables or `env.get_wrapper_attr('seed')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.seed to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.seed` for environment variables or `env.get_wrapper_attr('seed')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.seed to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.seed` for environment variables or `env.get_wrapper_attr('seed')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.seed to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.seed` for environment variables or `env.get_wrapper_attr('seed')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "test_envs = SubprocVectorEnv([lambda: gym.make(task) for _ in range(5)])\n",
    "test_envs.seed(seed);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_policy = MixedPolicy(ensemble, expert_policy, behavioral_critic, env.action_space, \"r\")\n",
    "mix_policy.update_novelty_threshold(offline_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/user/R901105/dev/my_fork/tianshou/tianshou/data/collector.py:70: UserWarning: Single environment detected, wrap to DummyVectorEnv.\n",
      "  warnings.warn(\"Single environment detected, wrap to DummyVectorEnv.\")\n"
     ]
    }
   ],
   "source": [
    "test_buffer = VectorReplayBuffer(5000, 5)\n",
    "train_collector = Collector(mix_policy, env, offline_data)\n",
    "test_collector = Collector(mix_policy, test_envs, test_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tianshou.utils import TensorboardLogger\n",
    "# log\n",
    "now = datetime.datetime.now().strftime(\"%y%m%d-%H%M%S\")\n",
    "log_name = os.path.join(task, \"dsac\", \"neutral\", now)\n",
    "log_path = os.path.join(\"../../log\", log_name)\n",
    "writer = SummaryWriter(log_path)\n",
    "logger = TensorboardLogger(writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_fn(num_epoch: int, step_idx: int):\n",
    "    if num_epoch > 0 :\n",
    "        mix_policy.update_novelty_threshold(offline_data)\n",
    "        print(test_buffer.policy.cede_ctrl.cpu().float().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1: 501it [00:12, 40.10it/s, env_step=500, gradient_step=500, len=0, loss/actor=-252.697, loss/critic1=0.645, loss/critic2=0.642, n/ep=0, n/st=1, rew=0.00]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "Epoch #1: test_reward: 3326.263896 ± 4.765138, best_reward: 3326.263896 ± 4.765138 in #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #2: 501it [00:12, 38.84it/s, env_step=1000, gradient_step=1000, len=1000, loss/actor=-253.765, loss/critic1=0.587, loss/critic2=0.585, n/ep=1, n/st=1, rew=3312.45]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "Epoch #2: test_reward: 3325.243856 ± 2.453645, best_reward: 3326.263896 ± 4.765138 in #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #3: 501it [00:14, 35.78it/s, env_step=1500, gradient_step=1500, len=1000, loss/actor=-253.833, loss/critic1=0.556, loss/critic2=0.557, n/ep=0, n/st=1, rew=3312.45]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "Epoch #3: test_reward: 3323.662852 ± 3.023935, best_reward: 3326.263896 ± 4.765138 in #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #4: 501it [00:12, 39.70it/s, env_step=2000, gradient_step=2000, len=1000, loss/actor=-255.347, loss/critic1=0.528, loss/critic2=0.530, n/ep=1, n/st=1, rew=3323.96]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "Epoch #4: test_reward: 3325.865963 ± 3.747576, best_reward: 3326.263896 ± 4.765138 in #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #5: 501it [00:12, 40.60it/s, env_step=2500, gradient_step=2500, len=1000, loss/actor=-256.491, loss/critic1=0.490, loss/critic2=0.487, n/ep=0, n/st=1, rew=3323.96]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "Epoch #5: test_reward: 3324.008282 ± 9.387396, best_reward: 3326.263896 ± 4.765138 in #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #6:  93%|#########3| 467/500 [00:11<00:00, 39.52it/s, env_step=2966, gradient_step=2966, len=1000, loss/actor=-258.055, loss/critic1=0.451, loss/critic2=0.449, n/ep=0, n/st=1, rew=3323.96]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 16\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtianshou\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OffpolicyTrainer\n\u001b[1;32m      3\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mOffpolicyTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffline_policy1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_collector\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_collector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_collector\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_collector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstep_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstep_per_collect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepisode_per_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mupdate_per_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_in_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m---> 16\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/user/R901105/dev/my_fork/tianshou/tianshou/trainer/base.py:444\u001b[0m, in \u001b[0;36mBaseTrainer.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_run \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 444\u001b[0m     \u001b[43mdeque\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxlen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# feed the entire iterator into a zero-length deque\u001b[39;00m\n\u001b[1;32m    445\u001b[0m     info \u001b[38;5;241m=\u001b[39m gather_info(\n\u001b[1;32m    446\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_time, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_collector, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_collector,\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_reward, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_reward_std\n\u001b[1;32m    448\u001b[0m     )\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m/data/user/R901105/dev/my_fork/tianshou/tianshou/trainer/base.py:302\u001b[0m, in \u001b[0;36mBaseTrainer.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    299\u001b[0m         result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn/st\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_step)\n\u001b[1;32m    300\u001b[0m         t\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m--> 302\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_update_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m     t\u001b[38;5;241m.\u001b[39mset_postfix(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdata)\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mn \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_fn_flag:\n",
      "File \u001b[0;32m/data/user/R901105/dev/my_fork/tianshou/tianshou/trainer/base.py:511\u001b[0m, in \u001b[0;36mOffpolicyTrainer.policy_update_fn\u001b[0;34m(self, data, result)\u001b[0m\n\u001b[1;32m    509\u001b[0m num_updates \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_per_step \u001b[38;5;241m*\u001b[39m n_collected_steps)\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_updates):\n\u001b[0;32m--> 511\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample_and_update\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_collector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/user/R901105/dev/my_fork/tianshou/tianshou/trainer/base.py:459\u001b[0m, in \u001b[0;36mBaseTrainer._sample_and_update\u001b[0;34m(self, buffer, data)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;66;03m# Note: since sample_size=batch_size, this will perform\u001b[39;00m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;66;03m# exactly one gradient step. This is why we don't need to calculate the\u001b[39;00m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;66;03m# number of gradient steps, like in the on-policy case.\u001b[39;00m\n\u001b[0;32m--> 459\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    460\u001b[0m data\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgradient_step\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_step)})\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_update_data(data, losses)\n",
      "File \u001b[0;32m/data/user/R901105/dev/my_fork/tianshou/tianshou/policy/base.py:346\u001b[0m, in \u001b[0;36mBasePolicy.update\u001b[0;34m(self, sample_size, buffer, **kwargs)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    345\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_fn(batch, buffer, indices)\n\u001b[0;32m--> 346\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_process_fn(batch, buffer, indices)\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/data/user/R901105/dev/my_fork/tianshou/tianshou/policy/modelfree/dsac.py:221\u001b[0m, in \u001b[0;36mDSACPolicy.learn\u001b[0;34m(self, batch, **kwargs)\u001b[0m\n\u001b[1;32m    217\u001b[0m taus_hat_j, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_taus(\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_taus, batch\u001b[38;5;241m.\u001b[39mobs\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    219\u001b[0m )\n\u001b[1;32m    220\u001b[0m critic1_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_critic_loss(batch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic1, taus_hat_j)\n\u001b[0;32m--> 221\u001b[0m critic2_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_critic_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcritic2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtaus_hat_j\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic1_optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    223\u001b[0m critic1_loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/data/user/R901105/dev/my_fork/tianshou/tianshou/policy/modelfree/dsac.py:178\u001b[0m, in \u001b[0;36mDSACPolicy._critic_loss\u001b[0;34m(self, batch, critic, taus_hat_j)\u001b[0m\n\u001b[1;32m    176\u001b[0m current_z \u001b[38;5;241m=\u001b[39m critic(batch\u001b[38;5;241m.\u001b[39mobs, batch\u001b[38;5;241m.\u001b[39mact, taus_hat_j)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 178\u001b[0m     target_z, presum_taus_i \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_target_z\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_quantile_regression_loss(\n\u001b[1;32m    181\u001b[0m     current_z, target_z, taus_hat_j, presum_taus_i\n\u001b[1;32m    182\u001b[0m )\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/data/user/R901105/dev/my_fork/tianshou/tianshou/policy/modelfree/dsac.py:158\u001b[0m, in \u001b[0;36mDSACPolicy._target_z\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    154\u001b[0m act_ \u001b[38;5;241m=\u001b[39m obs_next_result\u001b[38;5;241m.\u001b[39mact\n\u001b[1;32m    155\u001b[0m taus_hat, presum_taus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_taus(\n\u001b[1;32m    156\u001b[0m     batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_taus, batch\u001b[38;5;241m.\u001b[39mobs\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    157\u001b[0m )\n\u001b[0;32m--> 158\u001b[0m next_z1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcritic1_old\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobs_next\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mact_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtaus_hat\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (bsz, n_taus)\u001b[39;00m\n\u001b[1;32m    159\u001b[0m next_z2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic2_old(batch\u001b[38;5;241m.\u001b[39mobs_next, act_, taus_hat)\n\u001b[1;32m    160\u001b[0m log_prob \u001b[38;5;241m=\u001b[39m obs_next_result\u001b[38;5;241m.\u001b[39mlog_prob\u001b[38;5;241m.\u001b[39mrepeat((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_taus))\n",
      "File \u001b[0;32m/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data/user/R901105/dev/my_fork/tianshou/tianshou/utils/net/continuous.py:557\u001b[0m, in \u001b[0;36mQuantileMlp.forward\u001b[0;34m(self, state, action, tau)\u001b[0m\n\u001b[1;32m    554\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtau_fc(x)  \u001b[38;5;66;03m# (N, T, C)\u001b[39;00m\n\u001b[1;32m    556\u001b[0m h \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmul(x, h\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m))  \u001b[38;5;66;03m# (N, T, C)\u001b[39;00m\n\u001b[0;32m--> 557\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge_fc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (N, T, C)\u001b[39;00m\n\u001b[1;32m    558\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_fc(h)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (N, T)\u001b[39;00m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/torch/nn/modules/normalization.py:190\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/torch/nn/functional.py:2515\u001b[0m, in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[1;32m   2512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2513\u001b[0m         layer_norm, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, normalized_shape, weight\u001b[38;5;241m=\u001b[39mweight, bias\u001b[38;5;241m=\u001b[39mbias, eps\u001b[38;5;241m=\u001b[39meps\n\u001b[1;32m   2514\u001b[0m     )\n\u001b[0;32m-> 2515\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mlayer_norm(\u001b[38;5;28minput\u001b[39m, normalized_shape, weight, bias, eps, \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m)\n",
      "File \u001b[0;32m/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/torch/backends/__init__.py:31\u001b[0m, in \u001b[0;36mContextProp.__get__\u001b[0;34m(self, obj, objtype)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgetter \u001b[38;5;241m=\u001b[39m getter\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msetter \u001b[38;5;241m=\u001b[39m setter\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__get__\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj, objtype):\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgetter()\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__set__\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj, val):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tianshou.trainer import OffpolicyTrainer\n",
    "\n",
    "result = OffpolicyTrainer(\n",
    "    policy=offline_policy1,\n",
    "    train_collector=train_collector,\n",
    "    test_collector=test_collector,\n",
    "    test_fn=test_fn,\n",
    "    max_epoch=200,\n",
    "    step_per_epoch=500,\n",
    "    step_per_collect=1,\n",
    "    episode_per_test=5,\n",
    "    batch_size=256,\n",
    "    logger=logger,\n",
    "    update_per_step=1,\n",
    "    test_in_train=False,\n",
    ").run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch, _ = test_buffer.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Batch(\n",
       "    act: tensor([[-0.9850,  0.6664,  0.6509]], device='cuda:0',\n",
       "                grad_fn=<WhereBackward0>),\n",
       "    policy: Batch(\n",
       "                cede_ctrl: tensor([[False]], device='cuda:0'),\n",
       "            ),\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mix_policy(batch).policy.cede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_collector1 = Collector(offline_policy1, test_envs)\n",
    "test_collector1.collect(n_episode=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_collector2 = Collector(offline_policy2, test_envs)\n",
    "test_collector2.collect(n_episode=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_collector3 = Collector(offline_policy3, test_envs)\n",
    "test_collector3.collect(n_episode=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
