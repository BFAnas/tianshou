{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load offline trained agent\n",
    "# load expert agent\n",
    "# write ensemble class for offline trained agents\n",
    "# use distributional RL to detect risky states\n",
    "# use ensembles to detect novel states\n",
    "# if novelty is above a treshold give control to expert \n",
    "# if risk is above a treshold give control to expert\n",
    "# can conformal prediction give us guaranties about the performance in this setu?p\n",
    "# empirecally verify if we are able to get the desired performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from types import SimpleNamespace\n",
    "\n",
    "from examples.offline.utils import load_buffer_d4rl\n",
    "from tianshou.policy import DSACPolicy, BasePolicy\n",
    "from tianshou.env import SubprocVectorEnv\n",
    "from tianshou.data import Collector, Batch\n",
    "from tianshou.utils.net.common import Net\n",
    "from tianshou.utils.net.continuous import ActorProb, QuantileMlp\n",
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_value(value):\n",
    "    # Convert simple types (int, float, bool, None)\n",
    "    if value.isdigit():\n",
    "        return int(value)\n",
    "    elif re.match(r'^\\d+\\.\\d+$', value):\n",
    "        return float(value)\n",
    "    elif value == \"True\":\n",
    "        return True\n",
    "    elif value == \"False\":\n",
    "        return False\n",
    "    elif value == \"None\":\n",
    "        return None\n",
    "    elif value.startswith(\"[\") and value.endswith(\"]\"):\n",
    "        # Convert the list items\n",
    "        items = re.split(r',(?=[^\\]]*(?:\\[|$))', value[1:-1])\n",
    "        return [parse_value(item.strip()) for item in items]\n",
    "    elif value.startswith(\"(\") and value.endswith(\")\"):\n",
    "        # Convert the tuple items\n",
    "        items = re.split(r',(?=[^\\)]*(?:\\(|$))', value[1:-1])\n",
    "        # Special case for single-item tuple\n",
    "        if len(items) == 2 and items[0].strip() != '':\n",
    "            return (parse_value(items[0].strip()),)\n",
    "        return tuple(parse_value(item.strip()) for item in items)\n",
    "    elif value.startswith(\"'\") and value.endswith(\"'\"):\n",
    "        return value[1:-1]\n",
    "    # Else, return the value as-is\n",
    "    return value\n",
    "\n",
    "def get_args(event_file):\n",
    "    ea = EventAccumulator(event_file)\n",
    "    ea.Reload()  # Load the file\n",
    "    # Get the text data\n",
    "    texts = ea.Tags()[\"tensors\"]\n",
    "    # Extract the actual text content\n",
    "    text_data = {}\n",
    "    for tag in texts:\n",
    "        events = ea.Tensors(tag)\n",
    "        for event in events:\n",
    "            # You can extract the wall_time and step if needed\n",
    "            # wall_time, step, value = event.wall_time, event.step, event.text\n",
    "            text_data[tag] = event.tensor_proto.string_val\n",
    "    data = text_data['args/text_summary'][0]\n",
    "    # Convert bytes to string\n",
    "    data_str = data.decode('utf-8')\n",
    "    # Remove the \"Namespace(\" prefix and the trailing \")\"\n",
    "    data_str = data_str[len(\"Namespace(\"):-1]\n",
    "    # Split into key-value pairs\n",
    "    key_values = re.split(r',(?=\\s\\w+=)', data_str)\n",
    "    # Parse each key-value pair\n",
    "    args_dict = {}\n",
    "    for kv in key_values:\n",
    "        key, value = kv.split('=', 1)\n",
    "        key = key.strip()\n",
    "        args_dict[key] = parse_value(value)\n",
    "    args = SimpleNamespace(**args_dict)\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_policy(args, path):\n",
    "    env = gym.make(args.task)\n",
    "    # model\n",
    "    net_a = Net(args.state_shape, hidden_sizes=args.hidden_sizes, device=device)\n",
    "    actor = ActorProb(\n",
    "        net_a,\n",
    "        args.action_shape,\n",
    "        device=device,\n",
    "        unbounded=True,\n",
    "        conditioned_sigma=True,\n",
    "    ).to(device)\n",
    "    actor_optim = torch.optim.Adam(actor.parameters(), lr=args.actor_lr)\n",
    "    critic1 = QuantileMlp(hidden_sizes=args.hidden_sizes, input_size=args.state_shape[0] + args.action_shape[0], device=device).to(device)\n",
    "    critic1_optim = torch.optim.Adam(critic1.parameters(), lr=args.critic_lr)\n",
    "    critic2 = QuantileMlp(hidden_sizes=args.hidden_sizes, input_size=args.state_shape[0] + args.action_shape[0], device=device).to(device)\n",
    "    critic2_optim = torch.optim.Adam(critic2.parameters(), lr=args.critic_lr)\n",
    "    policy = DSACPolicy(\n",
    "        actor,\n",
    "        actor_optim,\n",
    "        critic1,\n",
    "        critic1_optim,\n",
    "        critic2,\n",
    "        critic2_optim,\n",
    "        risk_type=args.risk_type,\n",
    "        tau=args.tau,\n",
    "        gamma=args.gamma,\n",
    "        alpha=args.alpha,\n",
    "        action_space=env.action_space,\n",
    "        device=device,\n",
    "    )\n",
    "    dirname = os.path.dirname(path)\n",
    "    if os.path.isfile(os.path.join(dirname, \"actor.pth\")):\n",
    "        policy.actor.load_state_dict(torch.load(os.path.join(dirname, \"actor.pth\"), map_location=device))\n",
    "        print(\"Loaded actor from: \", os.path.join(dirname, \"actor.pth\"))\n",
    "    if os.path.isfile(os.path.join(dirname, \"critic1.pth\")):\n",
    "        policy.critic1.load_state_dict(torch.load(os.path.join(dirname, \"critic1.pth\"), map_location=device))\n",
    "        policy.critic1_old.load_state_dict(torch.load(os.path.join(dirname, \"critic1.pth\"), map_location=device))\n",
    "        print(\"Loaded critic1 from: \", os.path.join(dirname, \"critic1.pth\"))\n",
    "    if os.path.isfile(os.path.join(dirname, \"critic2.pth\")):\n",
    "        policy.critic2.load_state_dict(torch.load(os.path.join(dirname, \"critic2.pth\"), map_location=device))\n",
    "        policy.critic2_old.load_state_dict(torch.load(os.path.join(dirname, \"critic2.pth\"), map_location=device))\n",
    "        print(\"Loaded critic2 from: \", os.path.join(dirname, \"critic2.pth\"))\n",
    "    else:\n",
    "        policy.load_state_dict(torch.load(path, map_location=device))\n",
    "        print(\"Loaded agent from: \", path)\n",
    "    return policy\n",
    "\n",
    "def load_behavioral_crtitic(args, path):\n",
    "    behavioral_critic = QuantileMlp(\n",
    "        input_size=args.state_shape[0] + args.action_shape[0],\n",
    "        hidden_sizes=args.hidden_sizes,\n",
    "        device=device,\n",
    "    ).to(device)\n",
    "    behavioral_critic.load_state_dict(torch.load(path, map_location=device))\n",
    "    return behavioral_critic\n",
    "\n",
    "def get_model(log_path, type=None):\n",
    "    files = os.listdir(log_path)\n",
    "    event_file = [f for f in files if f.startswith('event')][0]\n",
    "    full_path = os.path.join(log_path, event_file)\n",
    "    args = get_args(full_path)\n",
    "    if type == \"behavioral\":\n",
    "        resume_path = os.path.join(log_path, 'model.pth')\n",
    "        policy = load_behavioral_crtitic(args, resume_path)\n",
    "    else:\n",
    "        resume_path = os.path.join(log_path, 'policy.pth')\n",
    "        policy = load_policy(args, resume_path)\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = \"/data/user/R901105/dev/log/Hopper-v4/qr/231102-133240\"\n",
    "behavioral_critic = get_model(log_path, \"behavioral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/gymnasium/envs/registration.py:513: DeprecationWarning: \u001b[33mWARN: The environment Hopper-v2 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
      "  logger.deprecation(\n",
      "/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/gymnasium/envs/mujoco/mujoco_env.py:211: DeprecationWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n",
      "  logger.deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded actor from:  /data/user/R901105/dev/log/Hopper-v2/codac_bc/neutral/0/231102-150037/actor.pth\n",
      "Loaded critic1 from:  /data/user/R901105/dev/log/Hopper-v2/codac_bc/neutral/0/231102-150037/critic1.pth\n",
      "Loaded critic2 from:  /data/user/R901105/dev/log/Hopper-v2/codac_bc/neutral/0/231102-150037/critic2.pth\n"
     ]
    }
   ],
   "source": [
    "log_path1 = \"/data/user/R901105/dev/log/Hopper-v2/codac_bc/neutral/0/231102-150037\"\n",
    "offline_policy1 = get_model(log_path1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded actor from:  /data/user/R901105/dev/log/Hopper-v2/codac_bc/wang/0/231106-104348/actor.pth\n",
      "Loaded critic1 from:  /data/user/R901105/dev/log/Hopper-v2/codac_bc/wang/0/231106-104348/critic1.pth\n",
      "Loaded critic2 from:  /data/user/R901105/dev/log/Hopper-v2/codac_bc/wang/0/231106-104348/critic2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/gymnasium/envs/registration.py:513: DeprecationWarning: \u001b[33mWARN: The environment Hopper-v2 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
      "  logger.deprecation(\n",
      "/data/user/R901105/.conda/envs/dev/lib/python3.11/site-packages/gymnasium/envs/mujoco/mujoco_env.py:211: DeprecationWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n",
      "  logger.deprecation(\n"
     ]
    }
   ],
   "source": [
    "log_path2 = \"/data/user/R901105/dev/log/Hopper-v2/codac_bc/wang/0/231106-104348\"\n",
    "offline_policy2 = get_model(log_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded actor from:  /data/user/R901105/dev/log/Hopper-v2/codac_bc/cvar/0/231106-123337/actor.pth\n",
      "Loaded critic1 from:  /data/user/R901105/dev/log/Hopper-v2/codac_bc/cvar/0/231106-123337/critic1.pth\n",
      "Loaded critic2 from:  /data/user/R901105/dev/log/Hopper-v2/codac_bc/cvar/0/231106-123337/critic2.pth\n"
     ]
    }
   ],
   "source": [
    "log_path3 = \"/data/user/R901105/dev/log/Hopper-v2/codac_bc/cvar/0/231106-123337\"\n",
    "offline_policy3 = get_model(log_path3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded agent from:  /data/user/R901105/dev/log/Hopper-v4/dsac/wang/0/230824-151635/policy.pth\n"
     ]
    }
   ],
   "source": [
    "log_path = \"/data/user/R901105/dev/log/Hopper-v4/dsac/wang/0/230824-151635\"\n",
    "expert_policy = get_model(log_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsemblePolicy(BasePolicy):\n",
    "    def __init__(self, policies, action_space):\n",
    "        super().__init__(action_space=action_space)\n",
    "        self.policies = policies\n",
    "\n",
    "    def forward(self, batch, state=None, **kwargs):\n",
    "        actions = np.stack([p(batch).act.detach().cpu() for p in self.policies])\n",
    "        return Batch(**{'act': np.mean(actions, axis=0), 'state': None})\n",
    "    \n",
    "    def learn(self, batch, **kwargs):\n",
    "        pass\n",
    "    \n",
    "    def get_qvalues(self, obs, act):\n",
    "        q_values = np.stack([p.critic1(obs, act).detach().cpu() for p in self.policies])\n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the performance of the expert policy\n",
    "task = \"Hopper-v2\"\n",
    "env = gym.make(task)\n",
    "envs = SubprocVectorEnv([lambda: gym.make(\"Hopper-v2\") for _ in range(20)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "policies = [offline_policy1, offline_policy2, offline_policy3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = EnsemblePolicy(policies, env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "collector = Collector(ensemble, envs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = collector.collect(n_episode=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1297.4056133 , 1551.75705752, 1525.07748803, 1543.24188126,\n",
       "       1560.76635264, 1574.4339472 , 1569.98205767, 1561.72085199,\n",
       "       1578.28293235, 1636.2818007 , 1645.35106032, 1774.9540754 ,\n",
       "       1787.26778757, 1772.41089845, 1827.39825956, 1819.17287196,\n",
       "       1996.27301949, 2267.44090986, 2377.69300312, 2413.05030823,\n",
       "       1417.60868587, 1549.67812415, 1535.43799899, 1660.32682601,\n",
       "       1762.30531851, 1722.73171207, 1526.29202326, 1807.54897506,\n",
       "       2121.68204262, 1805.1412413 , 1957.47295122, 1722.55052329,\n",
       "       1785.47652772, 2136.30970367, 1710.58438485, 2136.00623529,\n",
       "       2428.17527494, 2172.07740722, 2119.91707466, 2117.84973968,\n",
       "       1250.58047082, 1499.95465738, 1191.73644691, 1643.66161783,\n",
       "       1698.83964616, 1794.35831886, 1842.91518837, 1622.4076671 ,\n",
       "       1670.8247402 , 1826.863056  , 1465.48605068, 1728.47698258,\n",
       "       1249.89990589, 2124.01860294, 1581.46086299, 1552.62223346,\n",
       "       1804.64894351, 2611.88258341, 1595.56698652, 1588.84678055,\n",
       "       1525.64919364, 1698.53248213, 1569.68444674, 1542.07865949,\n",
       "       1575.85295009, 1670.39983728, 1796.36970151, 1674.41619122,\n",
       "       1762.52712609, 1841.50326393, 1553.69655684, 1762.74275223,\n",
       "       1804.48545629, 2037.23825328, 1718.53619004, 1557.52978611,\n",
       "       2179.75947868, 1523.52788243, 1498.79973925, 2455.24341034,\n",
       "       1656.31121594, 1758.8227374 , 1606.40596047, 2420.64457971,\n",
       "       1417.8924753 , 1959.89846627, 1872.17373385, 1768.38419806,\n",
       "       1301.79915819, 1539.98960732, 1546.67663725, 1982.88367149,\n",
       "       1549.22445653, 1833.8634253 , 1565.84348587, 2695.43390612,\n",
       "       2447.32394063, 1814.40532405, 2658.09636172, 2976.26359095])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['rews']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# establish baselines of the ensemble performance and the expert performance \n",
    "# based on epistemic and aleatoric uncertainties give control to the expert policy\n",
    "# check the performance of the ensemble + expert in comparaison with only the expert\n",
    "# perform continual learning and observe the evolution of numbers of calls to the expert"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
